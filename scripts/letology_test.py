#!/usr/bin/env python3
"""
Letology Full System Test Script

ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (Phase 1, 2, 3 ëª¨ë‘ ì‹¤í–‰)
- ë‹¨ì¼ CSV íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ ì§€ì›
- í´ë” ì…ë ¥ ì‹œ í•˜ìœ„ ëª¨ë“  CSV íŒŒì¼ ì½ê¸°
- ê²°ê³¼ë¥¼ íŒ”ë€í‹°ì–´ ìŠ¤íƒ€ì¼ë¡œ ìƒì„¸ ì¶œë ¥

Usage:
    python scripts/letology_test.py <input_path>
    python scripts/letology_test.py data/letsur_test_data/download_utf8.csv
    python scripts/letology_test.py data/letsur_test_data/
"""

import argparse
import asyncio
import json
import os
import sys
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


class LetologyTestRunner:
    """Letology ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ"""

    def __init__(self, input_path: str, output_dir: Optional[str] = None):
        self.input_path = Path(input_path).resolve()
        self.output_dir = Path(output_dir) if output_dir else PROJECT_ROOT / "test_results"
        self.start_time = None
        self.end_time = None
        self.results = {}

    def prepare_data_directory(self) -> Path:
        """
        ì…ë ¥ ê²½ë¡œë¥¼ ë°ì´í„° ë””ë ‰í† ë¦¬ë¡œ ì¤€ë¹„
        - ë‹¨ì¼ íŒŒì¼: ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
        - í´ë”: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        """
        if self.input_path.is_file():
            # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš° ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = self.output_dir / "temp_data"
            temp_dir.mkdir(parents=True, exist_ok=True)

            # íŒŒì¼ ë³µì‚¬
            dest = temp_dir / self.input_path.name
            shutil.copy2(self.input_path, dest)
            print(f"ğŸ“ Single file detected: {self.input_path.name}")
            print(f"   Copied to: {temp_dir}")
            return temp_dir

        elif self.input_path.is_dir():
            # í´ë”ì¸ ê²½ìš° CSV íŒŒì¼ ëª©ë¡ ì¶œë ¥
            csv_files = list(self.input_path.glob("*.csv"))
            print(f"ğŸ“ Directory detected: {self.input_path}")
            print(f"   Found {len(csv_files)} CSV files:")
            for f in csv_files[:5]:
                print(f"   - {f.name}")
            if len(csv_files) > 5:
                print(f"   ... and {len(csv_files) - 5} more")
            return self.input_path

        else:
            raise FileNotFoundError(f"Input path not found: {self.input_path}")

    def run_pipeline(self, data_dir: Path) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Phase 1, 2, 3) - subprocess ë°©ì‹"""
        import subprocess

        print("\n" + "=" * 70)
        print("ğŸš€ LETOLOGY FULL SYSTEM TEST")
        print("=" * 70)
        print(f"ğŸ“‚ Data Source: {data_dir}")
        print(f"ğŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)

        self.start_time = datetime.now()

        # Phase 1: Discovery
        print("\n" + "-" * 70)
        print("ğŸ“Š PHASE 1: DISCOVERY")
        print("-" * 70)
        print("   - Data Understanding (LLM-First Analysis)")
        print("   - TDA Analysis (Topological Data Analysis)")
        print("   - Schema Analysis")
        print("   - Value Matching")
        print("   - Entity Classification")
        print("   - Relationship Detection")

        # Phase 2: Refinement
        print("\n" + "-" * 70)
        print("ğŸ”§ PHASE 2: REFINEMENT")
        print("-" * 70)
        print("   - Ontology Architecture")
        print("   - Cross-Entity Correlation Analysis")
        print("   - Conflict Resolution")
        print("   - Quality Assessment")
        print("   - Semantic Validation")

        # Phase 3: Governance
        print("\n" + "-" * 70)
        print("ğŸ“‹ PHASE 3: GOVERNANCE")
        print("-" * 70)
        print("   - Governance Strategy")
        print("   - Action Prioritization")
        print("   - Risk Assessment")
        print("   - Policy Generation")

        print("\nâ³ Running pipeline... (this may take several minutes)")
        print("-" * 70)

        # subprocessë¡œ ì‹¤í–‰
        cmd = [
            sys.executable,
            "-m", "src.unified_pipeline.unified_main",
            str(data_dir),
        ]

        process = subprocess.run(
            cmd,
            cwd=str(PROJECT_ROOT),
            capture_output=False,
            text=True,
        )

        self.end_time = datetime.now()
        duration = (self.end_time - self.start_time).total_seconds()

        print("\n" + "=" * 70)
        print("âœ… PIPELINE COMPLETE")
        print("=" * 70)
        print(f"â±ï¸  Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)")
        print(f"ğŸ“… Completed: {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}")

        return {"returncode": process.returncode}

    def load_results(self) -> Dict[str, Any]:
        """ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ"""
        results = {}

        # ìµœì‹  ë¡œê·¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        logs_dir = PROJECT_ROOT / "Logs"
        if logs_dir.exists():
            log_dirs = sorted(logs_dir.iterdir(), key=lambda x: x.name, reverse=True)
            if log_dirs:
                latest_log = log_dirs[0]
                print(f"\nğŸ“‚ Results from: {latest_log}")

                # artifacts ë¡œë“œ
                artifacts_dir = latest_log / "artifacts"
                if artifacts_dir.exists():
                    for artifact in artifacts_dir.glob("*.json"):
                        with open(artifact, "r", encoding="utf-8") as f:
                            results[artifact.stem] = json.load(f)

                # summary ë¡œë“œ
                summary_file = latest_log / "summary.json"
                if summary_file.exists():
                    with open(summary_file, "r", encoding="utf-8") as f:
                        results["summary"] = json.load(f)

        return results

    def print_palantir_style_report(self, results: Dict[str, Any]):
        """íŒ”ë€í‹°ì–´ ìŠ¤íƒ€ì¼ë¡œ ê²°ê³¼ ì¶œë ¥"""
        print("\n")
        print("â•”" + "â•" * 78 + "â•—")
        print("â•‘" + " " * 20 + "ğŸ”® LETOLOGY ANALYSIS REPORT" + " " * 30 + "â•‘")
        print("â•‘" + " " * 15 + "Palantir Foundry Style Insights" + " " * 31 + "â•‘")
        print("â•š" + "â•" * 78 + "â•")

        # 1. Executive Summary
        self._print_executive_summary(results)

        # 2. Data Quality Assessment
        self._print_data_quality_section(results)

        # 3. Key Insights (Segment Analysis)
        self._print_key_insights_section(results)

        # 4. Discovered Entities & Relationships
        self._print_ontology_section(results)

        # 5. Recommended Actions
        self._print_actions_section(results)

        # 6. Risk Assessment
        self._print_risk_section(results)

    def _print_executive_summary(self, results: Dict[str, Any]):
        """Executive Summary ì¶œë ¥"""
        print("\n")
        print("â”Œ" + "â”€" * 78 + "â”")
        print("â”‚" + " ğŸ“Š EXECUTIVE SUMMARY".ljust(78) + "â”‚")
        print("â””" + "â”€" * 78 + "â”˜")

        summary = results.get("summary", {})
        insights = results.get("predictive_insights", [])
        ontology = results.get("ontology", {})

        # ê¸°ë³¸ í†µê³„
        print(f"""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Domain Detected    : {summary.get('domain', 'Unknown'):<52} â”‚
  â”‚  Execution Time     : {summary.get('duration_seconds', 0):.1f} seconds{' ' * 42}â”‚
  â”‚  Total Agents       : {summary.get('statistics', {}).get('total_agents', 0):<52} â”‚
  â”‚  LLM Calls          : {summary.get('statistics', {}).get('total_llm_calls', 0):<52} â”‚
  â”‚  Total Tokens       : {summary.get('statistics', {}).get('total_tokens', 0):,}{' ' * 43}â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

        # ì¸ì‚¬ì´íŠ¸ ìš”ì•½
        segment_insights = [i for i in insights if i.get('insight_type') == 'segment_analysis']
        quality_insights = [i for i in insights if i.get('insight_type') == 'data_quality_issue']

        print(f"  ğŸ“ˆ Total Insights Generated: {len(insights)}")
        print(f"     - Segment Analysis: {len(segment_insights)}")
        print(f"     - Data Quality Issues: {len(quality_insights)}")

        # ì—”í‹°í‹° ìš”ì•½
        entities = ontology.get("entities", [])
        relationships = ontology.get("relationships", [])
        print(f"\n  ğŸ”— Ontology Summary:")
        print(f"     - Entities Discovered: {len(entities)}")
        print(f"     - Relationships Found: {len(relationships)}")

    def _print_data_quality_section(self, results: Dict[str, Any]):
        """Data Quality ì„¹ì…˜ ì¶œë ¥"""
        print("\n")
        print("â”Œ" + "â”€" * 78 + "â”")
        print("â”‚" + " âš ï¸  DATA QUALITY ASSESSMENT".ljust(78) + "â”‚")
        print("â””" + "â”€" * 78 + "â”˜")

        insights = results.get("predictive_insights", [])
        quality_insights = [i for i in insights if i.get('insight_type') == 'data_quality_issue']

        if not quality_insights:
            print("\n  âœ… No significant data quality issues detected.")
            return

        print(f"\n  Found {len(quality_insights)} data quality issues:\n")

        for i, insight in enumerate(quality_insights[:5], 1):
            source_col = insight.get('source_column', 'Unknown')
            target_col = insight.get('target_column', '')
            condition = insight.get('source_condition', '')
            impact = insight.get('predicted_impact', '')

            # LLM íƒì§€ ì—¬ë¶€
            is_llm = "LLM íƒì§€" in condition or "[LLM ë¶„ì„]" in insight.get('business_interpretation', '')
            tag = "ğŸ¤– LLM" if is_llm else "ğŸ“Š STAT"

            print(f"  {tag} Issue #{i}: {source_col}")
            print(f"  â”œâ”€ Condition: {condition[:70]}{'...' if len(condition) > 70 else ''}")
            print(f"  â”œâ”€ Impact: {impact[:70]}{'...' if len(impact) > 70 else ''}")
            if target_col:
                print(f"  â””â”€ Related Column: {target_col}")
            else:
                print(f"  â””â”€ Severity: {insight.get('risk_assessment', 'Unknown')}")
            print()

    def _print_key_insights_section(self, results: Dict[str, Any]):
        """Key Insights (Segment Analysis) ì„¹ì…˜ ì¶œë ¥"""
        print("\n")
        print("â”Œ" + "â”€" * 78 + "â”")
        print("â”‚" + " ğŸ’¡ KEY BUSINESS INSIGHTS".ljust(78) + "â”‚")
        print("â””" + "â”€" * 78 + "â”˜")

        insights = results.get("predictive_insights", [])
        segment_insights = [i for i in insights if i.get('insight_type') == 'segment_analysis']

        if not segment_insights:
            print("\n  â„¹ï¸  No segment analysis insights available.")
            return

        print(f"\n  ğŸ“Š Segment Analysis Results ({len(segment_insights)} insights):\n")

        # ê·¸ë£¹í™”: metricë³„ë¡œ ì •ë¦¬
        metrics_seen = set()
        for insight in segment_insights:
            metric = insight.get('target_column', '')
            if metric in metrics_seen:
                continue
            metrics_seen.add(metric)

            interpretation = insight.get('business_interpretation', '')
            impact = insight.get('predicted_impact', '')
            recommendation = insight.get('recommended_action', '')
            risk = insight.get('risk_assessment', '')
            opportunity = insight.get('opportunity_assessment', '')
            concentration = insight.get('correlation_coefficient', 0) * 100

            print(f"  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
            print(f"  â•‘ ğŸ“ˆ Metric: {metric:<64} â•‘")
            print(f"  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")

            # ìƒìœ„ ì±„ë„ íŒŒì‹±
            if "ìƒìœ„ ìœ ì…ê²½ë¡œ:" in interpretation:
                channels_part = interpretation.split("ìƒìœ„ ìœ ì…ê²½ë¡œ:")[1].split(".")[0].strip()
                print(f"  â•‘ Top Channels: {channels_part:<60} â•‘")

            print(f"  â•‘ Concentration: {concentration:.1f}% ({impact.split('(')[1] if '(' in impact else ''}".ljust(75) + "â•‘")
            print(f"  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢")
            print(f"  â•‘ ğŸ¯ Recommendation: {recommendation[:55]}{'...' if len(recommendation) > 55 else '':<3} â•‘")
            print(f"  â•‘ âš ï¸  Risk: {risk:<65} â•‘")
            print(f"  â•‘ ğŸ’° Opportunity: {opportunity:<58} â•‘")
            print(f"  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            print()

    def _print_ontology_section(self, results: Dict[str, Any]):
        """Ontology ì„¹ì…˜ ì¶œë ¥"""
        print("\n")
        print("â”Œ" + "â”€" * 78 + "â”")
        print("â”‚" + " ğŸ”— DISCOVERED ONTOLOGY".ljust(78) + "â”‚")
        print("â””" + "â”€" * 78 + "â”˜")

        ontology = results.get("ontology", {})
        entities = ontology.get("entities", [])
        relationships = ontology.get("relationships", [])

        print(f"\n  Entities ({len(entities)}):")
        for entity in entities[:10]:
            # entityê°€ dictì¸ ê²½ìš°ì™€ stringì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            if isinstance(entity, dict):
                name = entity.get('name', entity.get('canonical_name', 'Unknown'))
                etype = entity.get('type', entity.get('entity_type', 'Unknown'))
                source = entity.get('source_tables', [])
                print(f"    â€¢ {name} ({etype})")
                if source:
                    print(f"      Source: {', '.join(source[:3])}")
            elif isinstance(entity, str):
                # ë¬¸ìì—´ í‘œí˜„ì—ì„œ ì •ë³´ ì¶”ì¶œ
                import re
                name_match = re.search(r"canonical_name='([^']*)'", entity)
                type_match = re.search(r"entity_type='([^']*)'", entity)
                source_match = re.search(r"source_tables=\[([^\]]*)\]", entity)

                name = name_match.group(1) if name_match else 'Unknown'
                etype = type_match.group(1) if type_match else 'Unknown'
                source_str = source_match.group(1) if source_match else ''

                print(f"    â€¢ {name} ({etype})")
                if source_str:
                    print(f"      Source: {source_str}")

        if relationships:
            print(f"\n  Relationships ({len(relationships)}):")
            for rel in relationships[:5]:
                print(f"    â€¢ {rel.get('source', '?')} â†’ {rel.get('target', '?')} ({rel.get('type', '?')})")

    def _print_actions_section(self, results: Dict[str, Any]):
        """Recommended Actions ì„¹ì…˜ ì¶œë ¥"""
        print("\n")
        print("â”Œ" + "â”€" * 78 + "â”")
        print("â”‚" + " ğŸ“‹ RECOMMENDED ACTIONS".ljust(78) + "â”‚")
        print("â””" + "â”€" * 78 + "â”˜")

        governance = results.get("governance_decisions", {})
        actions = governance.get("action_backlog", []) if isinstance(governance, dict) else []

        if not actions:
            print("\n  â„¹ï¸  No specific actions recommended.")
            return

        print(f"\n  Prioritized Action Items ({len(actions)}):\n")

        for i, action in enumerate(actions[:7], 1):
            title = action.get('title', action.get('action', 'Unknown'))
            priority = action.get('priority', 'Medium')
            impact = action.get('expected_impact', action.get('impact', 'Unknown'))

            priority_icon = "ğŸ”´" if priority == "High" else "ğŸŸ¡" if priority == "Medium" else "ğŸŸ¢"

            print(f"  {i}. {priority_icon} [{priority}] {title}")
            if impact and impact != 'Unknown':
                print(f"     Expected Impact: {impact[:60]}{'...' if len(str(impact)) > 60 else ''}")
            print()

    def _print_risk_section(self, results: Dict[str, Any]):
        """Risk Assessment ì„¹ì…˜ ì¶œë ¥"""
        print("\n")
        print("â”Œ" + "â”€" * 78 + "â”")
        print("â”‚" + " âš ï¸  RISK ASSESSMENT".ljust(78) + "â”‚")
        print("â””" + "â”€" * 78 + "â”˜")

        insights = results.get("predictive_insights", [])

        # ë¦¬ìŠ¤í¬ ìˆ˜ì§‘
        risks = []
        for insight in insights:
            risk = insight.get('risk_assessment', '')
            if risk and 'ë¦¬ìŠ¤í¬' in risk or 'risk' in risk.lower():
                risks.append({
                    'source': insight.get('source_column', 'Unknown'),
                    'risk': risk
                })

        if not risks:
            print("\n  âœ… No significant risks identified.")
            return

        print(f"\n  Identified Risks ({len(risks)}):\n")

        seen = set()
        for r in risks:
            if r['risk'] in seen:
                continue
            seen.add(r['risk'])
            print(f"  âš ï¸  {r['risk']}")
            print(f"     Related: {r['source']}")
            print()

    def run(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            # 1. ë°ì´í„° ì¤€ë¹„
            data_dir = self.prepare_data_directory()

            # 2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            self.run_pipeline(data_dir)

            # 3. ê²°ê³¼ ë¡œë“œ
            results = self.load_results()
            self.results = results

            # 4. íŒ”ë€í‹°ì–´ ìŠ¤íƒ€ì¼ ë¦¬í¬íŠ¸ ì¶œë ¥
            self.print_palantir_style_report(results)

            # 5. ì™„ë£Œ ë©”ì‹œì§€
            print("\n" + "=" * 80)
            print("ğŸ‰ LETOLOGY TEST COMPLETE!")
            print("=" * 80)
            print(f"\nğŸ“‚ Full results saved to: {PROJECT_ROOT / 'Logs'}")
            print(f"ğŸ“Š Pipeline results saved to: {self.output_dir}")

            return results

        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    parser = argparse.ArgumentParser(
        description="Letology Full System Test - Run complete pipeline on any dataset",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test with single CSV file
  python scripts/letology_test.py data/letsur_test_data/download_utf8.csv

  # Test with directory (all CSV files)
  python scripts/letology_test.py data/letsur_test_data/

  # Test with custom output directory
  python scripts/letology_test.py data/mydata.csv -o ./my_results
        """
    )

    parser.add_argument(
        "input_path",
        help="Path to CSV file or directory containing CSV files"
    )

    parser.add_argument(
        "-o", "--output",
        help="Output directory for results (default: ./test_results)",
        default=None
    )

    args = parser.parse_args()

    # ì…ë ¥ ê²½ë¡œ ê²€ì¦
    input_path = Path(args.input_path)
    if not input_path.exists():
        print(f"âŒ Error: Input path not found: {args.input_path}")
        sys.exit(1)

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    runner = LetologyTestRunner(args.input_path, args.output)
    runner.run()


if __name__ == "__main__":
    main()
