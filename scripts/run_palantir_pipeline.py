#!/usr/bin/env python3
"""
Palantir Dataset Full Pipeline Analysis (v2.0 - Agent-Based)

ì—ì´ì „íŠ¸ ê¸°ë°˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰:
1. ë°ì´í„° ë¡œë“œ ë° ì—ì´ì „íŠ¸ í¬ë§· ë³€í™˜
2. AutonomousPipelineOrchestratorë¥¼ í†µí•œ 3-Phase ì‹¤í–‰
   - Discovery: TDA, Schema, Value Matching, Entity, Relationship
   - Refinement: Ontology, Conflict Resolution, Quality, Validation
   - Governance: Strategy, Prioritization, Risk, Policy
3. ê²°ê³¼ ìˆ˜ì§‘ ë° ì €ì¥

Usage:
    python scripts/run_palantir_pipeline.py
    python scripts/run_palantir_pipeline.py --no-llm
    python scripts/run_palantir_pipeline.py --phases discovery refinement
"""

import os
import sys
import json
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import warnings
import logging

warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd

# Ontoloty Agent System ì„í¬íŠ¸
from src.unified_pipeline import (
    AutonomousPipelineOrchestrator,
    SharedContext,
)
from src.unified_pipeline.streaming.event_bus import (
    EventBus, DomainEvent, InsightEvents, create_event_bus
)
from src.unified_pipeline.enterprise.audit_logger import (
    AuditLogger, AuditEventType, ComplianceFramework, create_audit_logger
)

# LLM í´ë¼ì´ì–¸íŠ¸ (Optional)
try:
    from src.common.utils.llm import get_openai_client
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    get_openai_client = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================
# ë°ì´í„° ë¡œë“œ ë° ë³€í™˜
# ============================================================

DATA_DIR = PROJECT_ROOT / "data" / "palantir_test"
OUTPUT_DIR = PROJECT_ROOT / "data" / "pipeline_results"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_all_datasets() -> Dict[str, pd.DataFrame]:
    """ëª¨ë“  ë°ì´í„°ì…‹ ë¡œë“œ"""
    datasets = {}

    # General ë°ì´í„°ì…‹
    general_files = ["titanic.csv", "iris.csv", "flights.csv", "nyc_taxis.csv",
                     "diamonds.csv", "restaurant_tips.csv"]
    for f in general_files:
        path = DATA_DIR / f
        if path.exists():
            datasets[f.replace(".csv", "")] = pd.read_csv(path)
            print(f"  âœ“ Loaded {f}: {len(datasets[f.replace('.csv', '')])} rows")

    # Healthcare
    healthcare_files = ["hospitals.csv", "patients.csv"]
    for f in healthcare_files:
        path = DATA_DIR / "healthcare" / f
        if path.exists():
            key = f"healthcare_{f.replace('.csv', '')}"
            datasets[key] = pd.read_csv(path)
            print(f"  âœ“ Loaded healthcare/{f}: {len(datasets[key])} rows")

    # Manufacturing
    mfg_files = ["predictive_maintenance.csv", "iot_sensors.csv"]
    for f in mfg_files:
        path = DATA_DIR / "manufacturing" / f
        if path.exists():
            key = f"manufacturing_{f.replace('.csv', '')}"
            datasets[key] = pd.read_csv(path)
            print(f"  âœ“ Loaded manufacturing/{f}: {len(datasets[key])} rows")

    # Supply Chain
    sc_files = ["order_list.csv", "freight_rates.csv", "products_per_plant.csv",
                "plant_ports.csv", "warehouse_capacities.csv", "warehouse_costs.csv"]
    for f in sc_files:
        path = DATA_DIR / "supply_chain" / f
        if path.exists():
            key = f"supply_chain_{f.replace('.csv', '')}"
            datasets[key] = pd.read_csv(path)
            print(f"  âœ“ Loaded supply_chain/{f}: {len(datasets[key])} rows")

    return datasets


def convert_to_agent_format(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, Any]]:
    """
    DataFrameì„ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ìš”êµ¬í•˜ëŠ” í¬ë§·ìœ¼ë¡œ ë³€í™˜

    Required format:
    {
        "table_name": {
            "source": "palantir_test",
            "columns": [{"name": "col", "type": "int64", "nullable": True}, ...],
            "row_count": 1000,
            "sample_data": [{...}, ...],
            "metadata": {...},
            "primary_keys": [...],
            "foreign_keys": [...]
        }
    }
    """
    tables_data = {}

    for table_name, df in datasets.items():
        # ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
        columns = []
        for col in df.columns:
            col_info = {
                "name": col,
                "type": str(df[col].dtype),
                "nullable": bool(df[col].isnull().any()),
            }

            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            if df[col].dtype in ['int64', 'float64']:
                col_info["min"] = float(df[col].min()) if not df[col].isnull().all() else None
                col_info["max"] = float(df[col].max()) if not df[col].isnull().all() else None
            elif df[col].dtype == 'object':
                unique_count = df[col].nunique()
                col_info["unique_count"] = unique_count
                if unique_count <= 20:
                    col_info["sample_values"] = df[col].dropna().unique()[:10].tolist()

            columns.append(col_info)

        # Primary Key ì¶”ë¡  (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        primary_keys = []
        for col in df.columns:
            col_lower = col.lower()
            if col_lower == 'id' or col_lower.endswith('_id'):
                if df[col].nunique() == len(df):
                    primary_keys.append(col)
                    break

        # Foreign Key ì¶”ë¡  (íŒ¨í„´ ê¸°ë°˜)
        foreign_keys = []
        for col in df.columns:
            col_lower = col.lower()
            # FK íŒ¨í„´ ê°ì§€
            if '_id' in col_lower and col not in primary_keys:
                # ì°¸ì¡° í…Œì´ë¸” ì¶”ë¡ 
                base_name = col_lower.replace('_id', '')
                for other_table in datasets.keys():
                    if base_name in other_table.lower():
                        foreign_keys.append({
                            "source_column": col,
                            "target_table": other_table,
                            "target_column": "id",  # ì¶”ì •
                            "confidence": 0.7
                        })
                        break
            # Code íŒ¨í„´
            elif '_code' in col_lower:
                base_name = col_lower.replace('_code', '')
                for other_table in datasets.keys():
                    if base_name in other_table.lower():
                        foreign_keys.append({
                            "source_column": col,
                            "target_table": other_table,
                            "target_column": col,  # ê°™ì€ ì»¬ëŸ¼ëª…
                            "confidence": 0.6
                        })
                        break

        # ë„ë©”ì¸ ì¶”ë¡ 
        domain = "general"
        if "healthcare" in table_name:
            domain = "healthcare"
        elif "manufacturing" in table_name:
            domain = "manufacturing"
        elif "supply_chain" in table_name:
            domain = "supply_chain"

        tables_data[table_name] = {
            "source": "palantir_test",
            "domain": domain,
            "columns": columns,
            "row_count": len(df),
            "sample_data": df.head(500).to_dict('records'),
            "metadata": {
                "file_path": str(DATA_DIR / f"{table_name}.csv"),
                "loaded_at": datetime.now().isoformat(),
                "numeric_columns": df.select_dtypes(include=['number']).columns.tolist(),
                "categorical_columns": df.select_dtypes(include=['object']).columns.tolist(),
            },
            "primary_keys": primary_keys,
            "foreign_keys": foreign_keys,
        }

    return tables_data


# ============================================================
# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ============================================================

class PalantirAgentPipeline:
    """Palantir ë°ì´í„°ì…‹ì„ ìœ„í•œ ì—ì´ì „íŠ¸ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, use_llm: bool = True, phases: Optional[List[str]] = None):
        """
        Args:
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€ (Falseë©´ rule-based only)
            phases: ì‹¤í–‰í•  Phase ëª©ë¡ (Noneì´ë©´ ì „ì²´)
        """
        self.use_llm = use_llm and LLM_AVAILABLE
        self.phases = phases or ["discovery", "refinement", "governance"]

        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.llm_client = None
        if self.use_llm:
            try:
                self.llm_client = get_openai_client()
                logger.info("LLM client initialized (Letsur Gateway)")
            except Exception as e:
                logger.warning(f"LLM client initialization failed: {e}")
                self.use_llm = False

        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (ë°ì´í„° ë¡œë“œ í›„ ì´ˆê¸°í™”)
        self.orchestrator: Optional[AutonomousPipelineOrchestrator] = None

        # ê°ì‚¬ ë¡œê±°
        self.audit = create_audit_logger(
            storage_path=str(OUTPUT_DIR / "audit_logs")
        )

        # ì´ë²¤íŠ¸ ë²„ìŠ¤
        self.event_bus = create_event_bus()

        # ê²°ê³¼ ì €ì¥
        self.results = {
            "execution_time": None,
            "datasets_analyzed": 0,
            "total_rows": 0,
            "phases_completed": [],
            "agents_used": [],
            "insights": [],
            "relationships": [],
            "entities": [],
            "governance_actions": [],
        }

    def run(self) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = datetime.now()

        print("=" * 70)
        print("ONTOLOTY - Agent-Based Data Intelligence Pipeline (v2.0)")
        print("=" * 70)
        print(f"LLM Enabled: {self.use_llm}")
        print(f"Phases: {', '.join(self.phases)}")
        print("=" * 70)

        # 1. ë°ì´í„° ë¡œë“œ
        print("\n[Phase 0] Loading Datasets...")
        datasets = load_all_datasets()
        self.results["datasets_analyzed"] = len(datasets)
        self.results["total_rows"] = sum(len(df) for df in datasets.values())

        # 2. ì—ì´ì „íŠ¸ í¬ë§·ìœ¼ë¡œ ë³€í™˜
        print("\n[Phase 0.5] Converting to Agent Format...")
        tables_data = convert_to_agent_format(datasets)
        print(f"  âœ“ Converted {len(tables_data)} tables with metadata")

        # 3. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
        print("\n[Phase 1] Initializing Agent Orchestrator...")
        self.orchestrator = AutonomousPipelineOrchestrator(
            scenario_name="palantir_analysis",
            domain_context=None,  # ìë™ íƒì§€
            llm_client=self.llm_client,
            output_dir=str(OUTPUT_DIR),
            enable_continuation=True,
            max_concurrent_agents=5,
            auto_detect_domain=True,
        )
        print(f"  âœ“ Orchestrator initialized with {5} max concurrent agents")

        # 4. ê°ì‚¬ ë¡œê·¸ ì‹œì‘
        self.audit.log(
            AuditEventType.SYSTEM_START,
            actor="PalantirAgentPipeline",
            action="pipeline_start",
            resource="palantir_test",
            details={
                "datasets": list(tables_data.keys()),
                "use_llm": self.use_llm,
                "phases": self.phases,
            }
        )

        # 5. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë™ê¸° ëª¨ë“œ)
        print("\n[Phase 2-4] Running Agent Pipeline...")
        print("-" * 70)

        event_count = 0
        for event in self.orchestrator.run_sync(tables_data, phases=self.phases):
            event_type = event.get("type", "unknown")
            event_count += 1

            # ì´ë²¤íŠ¸ ì²˜ë¦¬
            self._handle_event(event)

        print("-" * 70)
        print(f"  âœ“ Processed {event_count} events")

        # 6. ê²°ê³¼ ìˆ˜ì§‘
        print("\n[Phase 5] Collecting Results...")
        self._collect_results()

        # 7. ì»´í”Œë¼ì´ì–¸ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±
        print("\n[Phase 6] Generating Compliance Reports...")
        self._generate_compliance_reports()

        # 8. ê²°ê³¼ ì €ì¥
        print("\n[Phase 7] Saving Results...")
        self._save_results()

        end_time = datetime.now()
        self.results["execution_time"] = str(end_time - start_time)

        # 9. ìš”ì•½ ì¶œë ¥
        self._print_summary()

        return self.results

    def _handle_event(self, event: Dict[str, Any]) -> None:
        """ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
        event_type = event.get("type", "unknown")

        if event_type == "domain_detection_start":
            print(f"  ğŸ” Auto-detecting domain...")

        elif event_type == "domain_detection_complete":
            domain = event.get("domain", "unknown")
            confidence = event.get("confidence", 0)
            print(f"  âœ“ Domain detected: {domain} (confidence: {confidence:.2%})")
            print(f"    Key entities: {event.get('key_entities', [])[:5]}")
            print(f"    Key metrics: {event.get('key_metrics', [])[:5]}")

        elif event_type == "phase_starting":
            phase = event.get("phase", "unknown")
            print(f"\n  â–¶ Starting {phase.upper()} phase...")

        elif event_type == "phase_completed":
            phase = event.get("phase", "unknown")
            progress = event.get("progress", 0)
            print(f"  âœ“ {phase.upper()} phase completed ({progress:.0%})")
            self.results["phases_completed"].append(phase)

        elif event_type == "todo_started":
            todo_name = event.get("name", "unknown")
            agent = event.get("agent_type", "unknown")
            print(f"    â†’ [{agent}] {todo_name}")
            if agent not in self.results["agents_used"]:
                self.results["agents_used"].append(agent)

        elif event_type == "todo_completed":
            todo_name = event.get("name", "unknown")
            duration = event.get("duration", 0)
            print(f"    âœ“ {todo_name} ({duration:.2f}s)")

        elif event_type == "todo_failed":
            todo_name = event.get("name", "unknown")
            error = event.get("error", "unknown")
            print(f"    âœ— {todo_name}: {error}")

        elif event_type == "pipeline_completed":
            stats = event.get("stats", {})
            print(f"\n  âœ“ Pipeline completed!")
            print(f"    Total agents created: {stats.get('total_agents_created', 0)}")
            print(f"    Total todos completed: {stats.get('total_todos_completed', 0)}")

        elif event_type == "pipeline_error":
            error = event.get("error", "unknown")
            print(f"\n  âœ— Pipeline error: {error}")

    def _collect_results(self) -> None:
        """SharedContextì—ì„œ ê²°ê³¼ ìˆ˜ì§‘"""
        if not self.orchestrator:
            return

        context = self.orchestrator.get_context()

        # TDA ì„œëª…
        if hasattr(context, 'tda_signatures'):
            self.results["tda_signatures"] = context.tda_signatures

        # ì—”í‹°í‹°
        if hasattr(context, 'unified_entities'):
            self.results["entities"] = [
                e.to_dict() if hasattr(e, 'to_dict') else str(e)
                for e in context.unified_entities
            ]

        # ê´€ê³„
        if hasattr(context, 'homeomorphisms'):
            self.results["relationships"] = [
                h.to_dict() if hasattr(h, 'to_dict') else str(h)
                for h in context.homeomorphisms
            ]

        # ì˜¨í†¨ë¡œì§€ ê°œë…
        if hasattr(context, 'ontology_concepts'):
            self.results["ontology_concepts"] = [
                c.to_dict() if hasattr(c, 'to_dict') else str(c)
                for c in context.ontology_concepts
            ]

        # ê±°ë²„ë„ŒìŠ¤ ê²°ì •
        if hasattr(context, 'governance_decisions'):
            self.results["governance_decisions"] = [
                d.to_dict() if hasattr(d, 'to_dict') else str(d)
                for d in context.governance_decisions
            ]

        # ì•¡ì…˜ ë°±ë¡œê·¸
        if hasattr(context, 'action_backlog'):
            self.results["governance_actions"] = context.action_backlog

        # ì¸ì‚¬ì´íŠ¸ (ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ)
        if hasattr(context, 'insights'):
            self.results["insights"] = context.insights

        print(f"  âœ“ Collected {len(self.results.get('entities', []))} entities")
        print(f"  âœ“ Collected {len(self.results.get('relationships', []))} relationships")
        print(f"  âœ“ Collected {len(self.results.get('governance_actions', []))} actions")

    def _generate_compliance_reports(self) -> None:
        """ì»´í”Œë¼ì´ì–¸ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±"""
        # SOX ë¦¬í¬íŠ¸
        sox_report = self.audit.generate_compliance_report(
            start_date=datetime.now() - timedelta(days=1),
            end_date=datetime.now() + timedelta(days=1),
            framework=ComplianceFramework.SOX
        )

        # GDPR ë¦¬í¬íŠ¸
        gdpr_report = self.audit.generate_compliance_report(
            start_date=datetime.now() - timedelta(days=1),
            end_date=datetime.now() + timedelta(days=1),
            framework=ComplianceFramework.GDPR
        )

        # ë¦¬í¬íŠ¸ ì €ì¥
        with open(OUTPUT_DIR / "sox_compliance_report.json", "w") as f:
            json.dump(sox_report, f, indent=2, default=str)

        with open(OUTPUT_DIR / "gdpr_compliance_report.json", "w") as f:
            json.dump(gdpr_report, f, indent=2, default=str)

        print(f"  âœ“ SOX Report: {sox_report.get('executive_summary', {}).get('total_events', 0)} events")
        print(f"  âœ“ GDPR Report generated")

    def _save_results(self) -> None:
        """ê²°ê³¼ ì €ì¥"""
        # ì „ì²´ ê²°ê³¼
        with open(OUTPUT_DIR / "pipeline_results.json", "w") as f:
            json.dump(self.results, f, indent=2, default=str)

        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì»¨í…ìŠ¤íŠ¸ (ì´ë¯¸ ì €ì¥ë¨)
        print(f"  âœ“ Results saved to {OUTPUT_DIR}")

    def _print_summary(self) -> None:
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("AGENT PIPELINE EXECUTION SUMMARY")
        print("=" * 70)

        print(f"\nğŸ“Š Data Processing:")
        print(f"   â€¢ Datasets Analyzed: {self.results['datasets_analyzed']}")
        print(f"   â€¢ Total Records: {self.results['total_rows']:,}")
        print(f"   â€¢ Execution Time: {self.results['execution_time']}")

        print(f"\nğŸ¤– Agents Used ({len(self.results['agents_used'])}):")
        for agent in self.results['agents_used']:
            print(f"   â€¢ {agent}")

        print(f"\nğŸ“‹ Phases Completed ({len(self.results['phases_completed'])}):")
        for phase in self.results['phases_completed']:
            print(f"   â€¢ {phase.upper()}")

        print(f"\nğŸ” Discovery Results:")
        print(f"   â€¢ Entities: {len(self.results.get('entities', []))}")
        print(f"   â€¢ Relationships: {len(self.results.get('relationships', []))}")

        print(f"\nğŸ¯ Governance Results:")
        print(f"   â€¢ Decisions: {len(self.results.get('governance_decisions', []))}")
        print(f"   â€¢ Actions: {len(self.results.get('governance_actions', []))}")

        print(f"\nğŸ“ Output Files:")
        for f in OUTPUT_DIR.glob("*.json"):
            size = f.stat().st_size
            print(f"   â€¢ {f.name} ({size:,} bytes)")

        print("\n" + "=" * 70)
        print("âœ… Agent Pipeline completed successfully!")
        print("=" * 70)


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    import argparse

    parser = argparse.ArgumentParser(description="Run Palantir Agent Pipeline")
    parser.add_argument("--no-llm", action="store_true", help="Disable LLM (rule-based only)")
    parser.add_argument("--phases", nargs="+",
                       choices=["discovery", "refinement", "governance"],
                       help="Phases to run (default: all)")

    args = parser.parse_args()

    use_llm = not args.no_llm
    phases = args.phases

    pipeline = PalantirAgentPipeline(use_llm=use_llm, phases=phases)
    results = pipeline.run()

    return results


if __name__ == "__main__":
    results = main()
