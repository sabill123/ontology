"""
Cross-Entity Correlation Analyzer for Palantir-Style Insights (v3.0 / v12.0)

v3.0 í•µì‹¬ ë³€ê²½: LLM ììœ¨ íŒë‹¨ ê°•í™”
- ìˆ˜í•™ì  ì œì•½ì¡°ê±´ ìë™ ê°ì§€ (A+B=1, A+B=100 ë“±)
- ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ ê°ì§€ (ëª¨ë°”ì¼_ì„¸ì…˜ìˆ˜ == ì „ì²´_ì„¸ì…˜ìˆ˜)
- ì¹´í…Œê³ ë¦¬/ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜ ë¶„ì„ ì¶”ê°€
- LLMì´ ë¬´ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¥¼ ìŠ¤ìŠ¤ë¡œ í•„í„°ë§

v2.0 í•µì‹¬ ë³€ê²½: LLM ììœ¨ ë¶„ì„
- í•˜ë“œì½”ë”©ëœ ê·œì¹™/í…œí”Œë¦¿ ì œê±°
- LLMì´ ë°ì´í„°ë¥¼ ë³´ê³  ììœ¨ì ìœ¼ë¡œ ì‹œë‚˜ë¦¬ì˜¤ íƒì§€, ê´€ê³„ ë°œê²¬, ì‹œë®¬ë ˆì´ì…˜ ì„¤ê³„
- ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ what-if ì‹œë®¬ë ˆì´ì…˜ ë° ê²€ì¦

íŒ”ë€í‹°ì–´ Foundry ìŠ¤íƒ€ì¼ì˜ Cross-Entity ì˜ˆì¸¡ ì¸ì‚¬ì´íŠ¸ ìƒì„±:
- "Gate 1 ìŠ¹ê° 30% ì¦ê°€ â†’ ë„ì¿„í–‰ ë¹„í–‰ê¸° ì§€ì—° í™•ë¥  20% ìƒìŠ¹"
- "Plant_A ì¬ê³  20% ê°ì†Œ â†’ Carrier_X ìš´ì†¡ë¹„ìš© 35% ì¦ê°€"
- "TPT_Day_Countê°€ 7ì¼ ì´ˆê³¼ì‹œ â†’ ë°°ì†¡ ì§€ì—° í™•ë¥  25% ìƒìŠ¹"
"""

import logging
import json
import uuid
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum

from ...model_config import get_service_model

import numpy as np

logger = logging.getLogger(__name__)


class InsightType(str, Enum):
    """íŒ”ë€í‹°ì–´ ì¸ì‚¬ì´íŠ¸ íƒ€ì…"""
    CROSS_ENTITY_IMPACT = "cross_entity_impact"
    THRESHOLD_ALERT = "threshold_alert"
    PREDICTIVE_CORRELATION = "predictive_correlation"
    CHAIN_EFFECT = "chain_effect"  # Aâ†’Bâ†’C ì²´ì¸ íš¨ê³¼
    ANOMALY_TRIGGER = "anomaly_trigger"  # ì´ìƒì¹˜ ë°œìƒ ì‹œ ì˜í–¥


@dataclass
class CorrelationResult:
    """í…Œì´ë¸” ê°„ ìƒê´€ê´€ê³„ ê²°ê³¼"""
    source_table: str
    source_column: str
    target_table: str
    target_column: str
    correlation: float
    p_value: float
    sample_size: int
    relationship_direction: str
    # v2.0: ì¶”ê°€ í†µê³„ ì •ë³´
    source_stats: Dict[str, float] = field(default_factory=dict)
    target_stats: Dict[str, float] = field(default_factory=dict)


@dataclass
class SimulationResult:
    """ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼"""
    scenario_id: str
    scenario_description: str
    source_change: Dict[str, Any]  # {"column": "X", "from": 100, "to": 120, "change_pct": 20}
    predicted_impacts: List[Dict[str, Any]]  # [{"column": "Y", "predicted_change": 15, "confidence": 0.85}]
    validation_score: float  # ì‹¤ì œ ë°ì´í„°ë¡œ ê²€ì¦í•œ ì •í™•ë„
    sample_evidence: List[Dict[str, Any]]  # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì°¾ì€ ì¦ê±°


@dataclass
class PalantirInsight:
    """íŒ”ë€í‹°ì–´ ìŠ¤íƒ€ì¼ ì˜ˆì¸¡ ì¸ì‚¬ì´íŠ¸ (v2.0)"""
    insight_id: str
    insight_type: str

    # ì›ì¸ ì¸¡
    source_table: str
    source_column: str
    source_condition: str

    # ê²°ê³¼ ì¸¡
    target_table: str
    target_column: str
    predicted_impact: str

    # í†µê³„ì  ê·¼ê±°
    correlation_coefficient: float
    p_value: float
    confidence: float
    sample_size: int

    # v2.0: LLM ìƒì„± ì½˜í…ì¸ 
    business_interpretation: str = ""
    recommended_action: str = ""
    risk_assessment: str = ""
    opportunity_assessment: str = ""

    # v2.0: ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
    simulation_results: List[Dict[str, Any]] = field(default_factory=list)
    validation_evidence: List[Dict[str, Any]] = field(default_factory=list)

    # ë©”íƒ€ë°ì´í„°
    source_entity: str = ""
    target_entity: str = ""
    chain_effects: List[str] = field(default_factory=list)  # Aâ†’Bâ†’C ì²´ì¸

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class CrossEntityCorrelationAnalyzer:
    """
    Cross-Entity ìƒê´€ê´€ê³„ ë¶„ì„ê¸° (v2.0 - LLM ììœ¨ ë¶„ì„)

    v2.0 í•µì‹¬ ë³€ê²½:
    - LLMì´ ììœ¨ì ìœ¼ë¡œ ì‹œë‚˜ë¦¬ì˜¤ íƒì§€/ì„¤ê³„
    - ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜
    - í•˜ë“œì½”ë”© ê·œì¹™ ì œê±°
    """

    def __init__(
        self,
        data_dir: str,
        correlation_threshold: float = 0.3,
        p_value_threshold: float = 0.05,
        llm_client=None,
    ):
        self.data_dir = Path(data_dir)
        self.correlation_threshold = correlation_threshold
        self.p_value_threshold = p_value_threshold
        self.llm_client = llm_client
        self._tables_data: Dict[str, Any] = {}
        self._column_stats: Dict[str, Dict[str, Dict[str, float]]] = {}  # table -> column -> stats

    def load_table_data(self, table_name: str) -> Optional[Any]:
        """CSV íŒŒì¼ì—ì„œ í…Œì´ë¸” ë°ì´í„° ë¡œë”©"""
        if table_name in self._tables_data:
            return self._tables_data[table_name]

        try:
            import pandas as pd

            csv_path = self.data_dir / f"{table_name}.csv"
            if not csv_path.exists():
                for path in self.data_dir.rglob(f"{table_name}.csv"):
                    csv_path = path
                    break

            if not csv_path.exists():
                logger.warning(f"CSV file not found: {table_name}")
                return None

            df = pd.read_csv(csv_path)
            self._tables_data[table_name] = df

            # ì»¬ëŸ¼ë³„ í†µê³„ ê³„ì‚° ë° ìºì‹œ
            self._compute_column_stats(table_name, df)

            logger.debug(f"Loaded table {table_name}: {len(df)} rows, {len(df.columns)} columns")
            return df

        except Exception as e:
            logger.warning(f"Failed to load table {table_name}: {e}")
            return None

    def _compute_column_stats(self, table_name: str, df: Any) -> None:
        """ì»¬ëŸ¼ë³„ í†µê³„ ê³„ì‚° (LLMì—ê²Œ ì œê³µí•  ì»¨í…ìŠ¤íŠ¸)"""
        import pandas as pd

        if table_name not in self._column_stats:
            self._column_stats[table_name] = {}

        # v12.0: ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ìºì‹œ
        if not hasattr(self, '_data_quality_issues'):
            self._data_quality_issues = []
        if not hasattr(self, '_mathematical_constraints'):
            self._mathematical_constraints = []
        if not hasattr(self, '_duplicate_columns'):
            self._duplicate_columns = []

        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

        for col in numeric_cols:
            series = df[col].dropna()
            if len(series) > 0:
                self._column_stats[table_name][col] = {
                    "mean": float(series.mean()),
                    "std": float(series.std()) if len(series) > 1 else 0.0,
                    "min": float(series.min()),
                    "max": float(series.max()),
                    "median": float(series.median()),
                    "q25": float(series.quantile(0.25)),
                    "q75": float(series.quantile(0.75)),
                    "count": int(len(series)),
                    "unique_ratio": float(series.nunique() / len(series)) if len(series) > 0 else 0,
                }

    def get_numeric_columns(self, df: Any) -> List[str]:
        """ìˆ«ìí˜• ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        filtered = [c for c in numeric_cols if not c.lower().endswith('_id')
                   and 'id' not in c.lower().split('_')]
        return filtered if filtered else numeric_cols[:10]

    # ============================================================
    # v12.0: ë°ì´í„° í’ˆì§ˆ ë° ìˆ˜í•™ì  ì œì•½ì¡°ê±´ ê°ì§€
    # ============================================================

    def _detect_mathematical_constraints(self, df: Any, table_name: str) -> List[Dict[str, Any]]:
        """
        v12.0: ìˆ˜í•™ì  ì œì•½ì¡°ê±´ ê°ì§€ (A+B=1, A+B=100 ë“±)

        ì˜ˆ: ì‹ ê·œë°©ë¬¸ë¹„ìœ¨ + ì¬ë°©ë¬¸ë¹„ìœ¨ = 1.0
        ì˜ˆ: ì™„ë£Œìœ¨ + ë¯¸ì™„ë£Œìœ¨ = 100

        ì´ëŸ° ê´€ê³„ëŠ” ìƒê´€ê´€ê³„ ë¶„ì„ì—ì„œ ì œì™¸í•´ì•¼ í•¨
        """
        import itertools

        constraints = []
        numeric_cols = self.get_numeric_columns(df)

        for col1, col2 in itertools.combinations(numeric_cols, 2):
            try:
                series1 = df[col1].dropna()
                series2 = df[col2].dropna()

                # ê°™ì€ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
                common_idx = series1.index.intersection(series2.index)
                if len(common_idx) < 10:
                    continue

                s1 = series1.loc[common_idx]
                s2 = series2.loc[common_idx]

                # í•©ì´ ìƒìˆ˜ì¸ì§€ í™•ì¸
                sum_values = s1 + s2
                sum_std = sum_values.std()
                sum_mean = sum_values.mean()

                # í‘œì¤€í¸ì°¨ê°€ ë§¤ìš° ì‘ìœ¼ë©´ (< í‰ê· ì˜ 1%) ìˆ˜í•™ì  ì œì•½
                if sum_mean != 0 and sum_std / abs(sum_mean) < 0.01:
                    # 1.0 ë˜ëŠ” 100 ê·¼ì²˜ì¸ì§€ í™•ì¸
                    is_unit_constraint = abs(sum_mean - 1.0) < 0.05
                    is_percent_constraint = abs(sum_mean - 100) < 5

                    if is_unit_constraint or is_percent_constraint:
                        constraint_type = "ë¹„ìœ¨ ë³´ì™„ (í•©=1)" if is_unit_constraint else "í¼ì„¼íŠ¸ ë³´ì™„ (í•©=100)"
                        constraints.append({
                            "table": table_name,
                            "column_1": col1,
                            "column_2": col2,
                            "constraint_type": constraint_type,
                            "sum_value": float(sum_mean),
                            "reason": f"{col1} + {col2} = {sum_mean:.2f} (ìˆ˜í•™ì  ì •ì˜ ê´€ê³„)"
                        })
                        logger.info(f"[v12.0] Mathematical constraint detected: {col1} + {col2} = {sum_mean:.2f}")

            except Exception as e:
                logger.debug(f"Constraint check failed for {col1}, {col2}: {e}")

        return constraints

    def _detect_duplicate_columns(self, df: Any, table_name: str) -> List[Dict[str, Any]]:
        """
        v12.0: ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ ê°ì§€

        ì˜ˆ: ëª¨ë°”ì¼_ì„¸ì…˜ìˆ˜ == ì „ì²´_ì„¸ì…˜ìˆ˜ (100% ë™ì¼)

        ì´ëŸ° ê²½ìš° ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë¡œ ë³´ê³ í•´ì•¼ í•¨
        """
        import itertools

        duplicates = []
        numeric_cols = self.get_numeric_columns(df)

        for col1, col2 in itertools.combinations(numeric_cols, 2):
            try:
                series1 = df[col1].dropna()
                series2 = df[col2].dropna()

                common_idx = series1.index.intersection(series2.index)
                if len(common_idx) < 10:
                    continue

                s1 = series1.loc[common_idx]
                s2 = series2.loc[common_idx]

                # ì™„ì „ ì¼ì¹˜ í™•ì¸
                match_rate = (s1 == s2).mean()

                if match_rate > 0.99:  # 99% ì´ìƒ ì¼ì¹˜
                    duplicates.append({
                        "table": table_name,
                        "column_1": col1,
                        "column_2": col2,
                        "match_rate": float(match_rate),
                        "severity": "warning",
                        "reason": f"'{col1}'ê³¼ '{col2}'ê°€ {match_rate*100:.1f}% ë™ì¼í•©ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜ ë˜ëŠ” ì¤‘ë³µ ì»¬ëŸ¼ ê°€ëŠ¥ì„±."
                    })
                    logger.warning(f"[v12.0] Duplicate columns detected: {col1} == {col2} ({match_rate*100:.1f}%)")

            except Exception as e:
                logger.debug(f"Duplicate check failed for {col1}, {col2}: {e}")

        return duplicates

    def _detect_derived_columns(self, df: Any, table_name: str) -> List[Dict[str, Any]]:
        """
        v26.0: íŒŒìƒ(ê³„ì‚°) ì»¬ëŸ¼ ê°ì§€

        col_c â‰ˆ col_a / col_b * K (K=1 or 100) í˜•íƒœì˜ ê´€ê³„ë¥¼ ìë™ íƒì§€.
        ì˜ˆ: like_rate = like_count / view_count * 100
        ì˜ˆ: views_per_follower = view_count / follower_count
        ì˜ˆ: avg_scene_duration = duration / scene_count
        """
        import itertools

        derived = []
        numeric_cols = self.get_numeric_columns(df)

        if len(numeric_cols) < 3:
            return derived

        for target_col in numeric_cols:
            target = df[target_col].dropna()
            if len(target) < 20:
                continue

            for col_a, col_b in itertools.combinations(numeric_cols, 2):
                if col_a == target_col or col_b == target_col:
                    continue

                try:
                    sa = df[col_a].loc[target.index].dropna()
                    sb = df[col_b].loc[target.index].dropna()
                    common = target.index.intersection(sa.index).intersection(sb.index)
                    if len(common) < 20:
                        continue

                    t = target.loc[common]
                    a = sa.loc[common]
                    b = sb.loc[common]

                    # b != 0 ì¸ í–‰ë§Œ ì‚¬ìš© (division by zero ë°©ì§€)
                    nonzero_mask = b != 0
                    if nonzero_mask.sum() < 20:
                        continue

                    t_nz = t[nonzero_mask]
                    a_nz = a[nonzero_mask]
                    b_nz = b[nonzero_mask]

                    # a/bì™€ ë¹„êµ
                    ratio = a_nz / b_nz

                    # K=1: target â‰ˆ a / b
                    for multiplier, label in [(1.0, ""), (100.0, " * 100")]:
                        computed = ratio * multiplier
                        # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚° (tê°€ 0ì¸ ê²½ìš° ì ˆëŒ€ ì˜¤ì°¨ ì‚¬ìš©)
                        abs_diff = (t_nz - computed).abs()
                        abs_target = t_nz.abs().clip(lower=1e-10)
                        rel_error = (abs_diff / abs_target).median()

                        if rel_error < 0.02:  # ì¤‘ìœ„ ìƒëŒ€ì˜¤ì°¨ 2% ë¯¸ë§Œ
                            match_rate = float((abs_diff / abs_target < 0.05).mean())
                            if match_rate >= 0.95:  # 95% ì´ìƒ ì¼ì¹˜
                                formula = f"{col_a} / {col_b}{label}"
                                derived.append({
                                    "table": table_name,
                                    "derived_column": target_col,
                                    "formula": formula,
                                    "source_columns": [col_a, col_b],
                                    "match_rate": match_rate,
                                    "sample_size": int(nonzero_mask.sum()),
                                    "severity": "info",
                                    "reason": f"'{target_col}' = {formula} (ì¼ì¹˜ìœ¨ {match_rate*100:.1f}%, íŒŒìƒ ì»¬ëŸ¼)",
                                })
                                logger.info(f"[v26.0] Derived column detected: {target_col} = {formula} ({match_rate*100:.1f}% match)")
                                break  # ì´ target_colì— ëŒ€í•´ ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ ì‚¬ìš©
                    else:
                        continue
                    break  # target_colì— ëŒ€í•œ ì²« ë§¤ì¹­ì„ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë‹¤ìŒ targetìœ¼ë¡œ

                except Exception as e:
                    logger.debug(f"Derived column check failed for {target_col} = {col_a}/{col_b}: {e}")

        return derived

    def _detect_categorical_columns(self, df: Any) -> List[str]:
        """v12.0: ì¹´í…Œê³ ë¦¬/ì„¸ê·¸ë¨¼íŠ¸ ì»¬ëŸ¼ ê°ì§€"""
        import pandas as pd
        categorical_cols = []

        for col in df.columns:
            # ë¬¸ìì—´ ì»¬ëŸ¼ì´ë©´ì„œ ê³ ìœ ê°’ì´ ì ë‹¹íˆ ìˆëŠ” ê²½ìš°
            if pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):
                nunique = df[col].nunique()
                if 2 <= nunique <= 100:  # 2~100ê°œ ê³ ìœ ê°’
                    categorical_cols.append(col)

        return categorical_cols

    def detect_confounding_variables(
        self, df: Any, treatment_col: str, outcome_col: str,
        candidate_confounders: List[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        v26.0: êµë€ë³€ìˆ˜(confounding variable) íƒì§€ â€” Simpson's Paradox ê²€ì¶œ

        treatment_colì´ outcome_colì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ê°€ confounderë¥¼ í†µì œí•˜ë©´
        ì‚¬ë¼ì§€ê±°ë‚˜ ë°˜ì „ë˜ëŠ”ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤.

        Returns: êµë€ë³€ìˆ˜ í›„ë³´ ë¦¬ìŠ¤íŠ¸ [{confounder, raw_effect, adjusted_effect, is_confounded}]
        """
        from scipy import stats

        confounders_found = []

        if candidate_confounders is None:
            candidate_confounders = self.get_numeric_columns(df)

        # ì›ì‹œ ìƒê´€ê´€ê³„
        try:
            mask = df[[treatment_col, outcome_col]].notna().all(axis=1)
            raw_corr, raw_p = stats.pearsonr(
                df.loc[mask, treatment_col], df.loc[mask, outcome_col]
            )
        except Exception:
            return confounders_found

        for confounder in candidate_confounders:
            if confounder in (treatment_col, outcome_col):
                continue

            try:
                # 3ë³€ìˆ˜ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” í–‰ë§Œ ì‚¬ìš©
                valid = df[[treatment_col, outcome_col, confounder]].dropna()
                if len(valid) < 30:
                    continue

                # í¸ìƒê´€(partial correlation): treatment-outcome ìƒê´€ì—ì„œ confounder íš¨ê³¼ ì œê±°
                # rAB.C = (rAB - rAC * rBC) / sqrt((1 - rAC^2)(1 - rBC^2))
                r_to = raw_corr
                r_tc, _ = stats.pearsonr(valid[treatment_col], valid[confounder])
                r_oc, _ = stats.pearsonr(valid[outcome_col], valid[confounder])

                denom = ((1 - r_tc**2) * (1 - r_oc**2)) ** 0.5
                if denom < 1e-10:
                    continue
                partial_corr = (r_to - r_tc * r_oc) / denom

                # êµë€ íŒì •: ì›ì‹œ ìƒê´€ì´ ìœ ì˜ë¯¸í•˜ì§€ë§Œ í¸ìƒê´€ì´ 50% ì´ìƒ ê°ì†Œ
                raw_abs = abs(raw_corr)
                partial_abs = abs(partial_corr)
                reduction = (raw_abs - partial_abs) / raw_abs if raw_abs > 0.05 else 0

                if reduction > 0.4 and raw_abs > 0.05:
                    # Simpson's paradox ì²´í¬: ì¹´í…Œê³ ë¦¬í˜• treatmentì¼ ë•Œ ì¸µí™” ë¶„ì„
                    is_simpson = False
                    if df[treatment_col].nunique() <= 5:
                        # ì¸µí™” ë¶„ì„ â€” confounderë¥¼ quantileë¡œ ë‚˜ëˆ„ì–´ ê° ì¸µì—ì„œ íš¨ê³¼ í™•ì¸
                        try:
                            valid["_conf_bin"] = np.digitize(
                                valid[confounder],
                                bins=np.quantile(valid[confounder].dropna(), [0.33, 0.67]),
                            )
                            strata_effects = []
                            for _, stratum in valid.groupby("_conf_bin"):
                                if len(stratum) < 10:
                                    continue
                                s_corr, _ = stats.pearsonr(
                                    stratum[treatment_col], stratum[outcome_col]
                                )
                                strata_effects.append(s_corr)
                            # ì¸µí™” íš¨ê³¼ì˜ ë¶€í˜¸ê°€ ì›ì‹œì™€ ë°˜ëŒ€ â†’ Simpson's Paradox
                            if strata_effects and all(
                                (e * raw_corr) < 0 for e in strata_effects if abs(e) > 0.02
                            ):
                                is_simpson = True
                        except Exception:
                            pass

                    confounders_found.append({
                        "confounder": confounder,
                        "raw_correlation": round(float(raw_corr), 4),
                        "partial_correlation": round(float(partial_corr), 4),
                        "reduction_pct": round(float(reduction * 100), 1),
                        "r_treatment_confounder": round(float(r_tc), 4),
                        "r_outcome_confounder": round(float(r_oc), 4),
                        "is_confounded": True,
                        "is_simpsons_paradox": is_simpson,
                        "note": (
                            f"'{confounder}'ë¥¼ í†µì œí•˜ë©´ {treatment_col}â†’{outcome_col} "
                            f"ìƒê´€ì´ {raw_abs:.3f}â†’{partial_abs:.3f}ë¡œ {reduction*100:.0f}% ê°ì†Œ"
                            + (" [Simpson's Paradox ê°ì§€]" if is_simpson else "")
                        ),
                    })
                    logger.info(
                        f"[v26.0] Confounding detected: {confounder} confounds "
                        f"{treatment_col}â†’{outcome_col} (reduction={reduction*100:.0f}%"
                        f"{', Simpson!' if is_simpson else ''})"
                    )

            except Exception as e:
                logger.debug(f"Confounding check failed for {confounder}: {e}")

        return confounders_found

    def _analyze_categorical_insights(self, df: Any, table_name: str) -> List[Dict[str, Any]]:
        """
        v12.2: ì¹´í…Œê³ ë¦¬/ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ (ê°•í™”)

        ì˜ˆ: íŠ¸ë˜í”½ ì†ŒìŠ¤ë³„ ì„¸ì…˜ìˆ˜, ì²´ë¥˜ì‹œê°„, ì‹ ê·œë°©ë¬¸ë¹„ìœ¨ ë¶„ì„
        """
        insights = []
        categorical_cols = self._detect_categorical_columns(df)
        numeric_cols = self.get_numeric_columns(df)

        logger.info(f"[v12.2] Analyzing categorical columns: {categorical_cols}")
        logger.info(f"[v12.2] Numeric columns for segment analysis: {numeric_cols[:5]}")

        # í•µì‹¬ ì§€í‘œ ìš°ì„ ìˆœìœ„ (ì„¸ì…˜ìˆ˜, ì²´ë¥˜ì‹œê°„, ë¹„ìœ¨ ë“±)
        priority_patterns = ['ì„¸ì…˜', 'session', 'ì²´ë¥˜', 'time', 'duration', 'ë°©ë¬¸', 'visit']

        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬
        def get_priority(col):
            col_lower = col.lower()
            for i, pattern in enumerate(priority_patterns):
                if pattern in col_lower:
                    return i
            return len(priority_patterns)

        sorted_numeric = sorted(numeric_cols, key=get_priority)

        for cat_col in categorical_cols[:3]:  # v12.2: ìµœëŒ€ 3ê°œ ì¹´í…Œê³ ë¦¬
            for num_col in sorted_numeric[:5]:  # v12.2: ìµœëŒ€ 5ê°œ ì§€í‘œ
                try:
                    # ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
                    agg = df.groupby(cat_col)[num_col].agg(['sum', 'mean', 'count', 'std'])

                    if len(agg) < 2:
                        continue

                    # TOP 5 ë¶„ì„ (sum ê¸°ì¤€)
                    top_5_sum = agg['sum'].nlargest(5)
                    total = agg['sum'].sum()

                    if total == 0:
                        continue

                    top_categories = []
                    for cat, value in top_5_sum.items():
                        pct = (value / total) * 100
                        cat_mean = agg.loc[cat, 'mean']
                        cat_count = agg.loc[cat, 'count']
                        top_categories.append({
                            "category": str(cat),
                            "value": float(value),
                            "percentage": float(pct),
                            "mean": float(cat_mean),
                            "count": int(cat_count)
                        })

                    concentration = sum(c["percentage"] for c in top_categories)

                    # v12.2: í‰ê·  ê¸°ì¤€ TOP 5ë„ ë¶„ì„ (ì²´ë¥˜ì‹œê°„ ë“±ì— ìœ ìš©)
                    top_5_mean = agg['mean'].nlargest(5)
                    top_by_mean = []
                    for cat, mean_val in top_5_mean.items():
                        if agg.loc[cat, 'count'] >= 3:  # ìµœì†Œ 3ê°œ ìƒ˜í”Œ
                            top_by_mean.append({
                                "category": str(cat),
                                "mean": float(mean_val),
                                "count": int(agg.loc[cat, 'count'])
                            })

                    insights.append({
                        "table": table_name,
                        "category_column": cat_col,
                        "metric_column": num_col,
                        "total_categories": int(df[cat_col].nunique()),
                        "top_categories": top_categories,
                        "top_by_mean": top_by_mean[:5],  # í‰ê·  ê¸°ì¤€ TOP 5
                        "top5_concentration": float(concentration),
                        "overall_mean": float(agg['mean'].mean()),
                        "overall_std": float(agg['mean'].std()) if len(agg) > 1 else 0.0,
                        "insight_type": "category_distribution"
                    })

                    logger.debug(f"[v12.2] Segment insight: {cat_col} x {num_col}, "
                                f"top={top_categories[0]['category'] if top_categories else 'N/A'}, "
                                f"concentration={concentration:.1f}%")

                except Exception as e:
                    logger.debug(f"Categorical analysis failed for {cat_col} x {num_col}: {e}")

        logger.info(f"[v12.2] Generated {len(insights)} categorical insights for {table_name}")
        return insights

    def compute_correlation(
        self,
        series_a: Any,
        series_b: Any,
    ) -> Tuple[float, float, int]:
        """ë‘ ì‹œë¦¬ì¦ˆ ê°„ Pearson ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        from scipy import stats
        import pandas as pd

        combined = pd.concat([series_a, series_b], axis=1).dropna()

        if len(combined) < 10:
            return 0.0, 1.0, 0

        try:
            corr, p_value = stats.pearsonr(combined.iloc[:, 0], combined.iloc[:, 1])
            return float(corr), float(p_value), len(combined)
        except Exception:
            return 0.0, 1.0, 0

    def compute_cross_table_correlations(
        self,
        table_names: List[str],
    ) -> List[CorrelationResult]:
        """í…Œì´ë¸” ê°„ ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ ìŒì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        import pandas as pd

        correlations = []
        tables_data = {}

        for name in table_names:
            df = self.load_table_data(name)
            if df is not None and len(df) > 0:
                tables_data[name] = df

        if len(tables_data) < 2:
            # v24.0: WARNING â†’ DEBUG (ë‹¨ì¼ í…Œì´ë¸”ì—ì„œëŠ” ì •ìƒ ë™ì‘)
            logger.debug("Need at least 2 tables for cross-table correlation")
            return correlations

        table_list = list(tables_data.keys())

        for i, table_a in enumerate(table_list):
            df_a = tables_data[table_a]
            cols_a = self.get_numeric_columns(df_a)

            for table_b in table_list[i+1:]:
                df_b = tables_data[table_b]
                cols_b = self.get_numeric_columns(df_b)

                min_rows = min(len(df_a), len(df_b))

                # v8.1: skip if row counts too different (spurious correlation risk)
                if min_rows < 10 or min_rows / max(len(df_a), len(df_b), 1) < 0.8:
                    continue

                for col_a in cols_a[:10]:
                    for col_b in cols_b[:10]:
                        if col_a == col_b:
                            continue

                        series_a = df_a[col_a].iloc[:min_rows].reset_index(drop=True)
                        series_b = df_b[col_b].iloc[:min_rows].reset_index(drop=True)

                        corr, p_value, sample_size = self.compute_correlation(series_a, series_b)

                        if abs(corr) >= self.correlation_threshold and p_value <= self.p_value_threshold:
                            # v2.0: í†µê³„ ì •ë³´ ì¶”ê°€
                            source_stats = self._column_stats.get(table_a, {}).get(col_a, {})
                            target_stats = self._column_stats.get(table_b, {}).get(col_b, {})

                            correlations.append(CorrelationResult(
                                source_table=table_a,
                                source_column=col_a,
                                target_table=table_b,
                                target_column=col_b,
                                correlation=corr,
                                p_value=p_value,
                                sample_size=sample_size,
                                relationship_direction="positive" if corr > 0 else "negative",
                                source_stats=source_stats,
                                target_stats=target_stats,
                            ))

        # ìƒê´€ê´€ê³„ ê°•ë„ë¡œ ì •ë ¬
        correlations.sort(key=lambda x: abs(x.correlation), reverse=True)

        logger.info(f"Found {len(correlations)} significant cross-table correlations")
        return correlations

    def _prepare_data_summary_for_llm(self, tables_data: Dict[str, Any]) -> str:
        """LLMì—ê²Œ ì „ë‹¬í•  ë°ì´í„° ìš”ì•½ ìƒì„± (v12.0: ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ í¬í•¨)"""
        summary_parts = []

        # v12.0: ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ìˆ˜ì§‘
        all_constraints = []
        all_duplicates = []
        all_categorical = []
        all_derived = []  # v26.0: íŒŒìƒ ì»¬ëŸ¼

        for table_name, df in tables_data.items():
            numeric_cols = self.get_numeric_columns(df)
            stats = self._column_stats.get(table_name, {})

            col_info = []
            for col in numeric_cols[:8]:  # ì£¼ìš” ì»¬ëŸ¼ë§Œ
                col_stats = stats.get(col, {})
                if col_stats:
                    col_info.append(
                        f"  - {col}: mean={col_stats.get('mean', 0):.2f}, "
                        f"std={col_stats.get('std', 0):.2f}, "
                        f"range=[{col_stats.get('min', 0):.2f}, {col_stats.get('max', 0):.2f}]"
                    )

            summary_parts.append(
                f"í…Œì´ë¸”: {table_name} ({len(df)} rows)\n"
                f"ìˆ«ìí˜• ì»¬ëŸ¼:\n" + "\n".join(col_info)
            )

            # v12.0: ê° í…Œì´ë¸”ì— ëŒ€í•´ í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰
            constraints = self._detect_mathematical_constraints(df, table_name)
            duplicates = self._detect_duplicate_columns(df, table_name)
            categorical = self._analyze_categorical_insights(df, table_name)
            # v26.0: íŒŒìƒ ì»¬ëŸ¼ ê°ì§€
            derived = self._detect_derived_columns(df, table_name)

            all_constraints.extend(constraints)
            all_duplicates.extend(duplicates)
            all_categorical.extend(categorical)
            all_derived.extend(derived)

        # ìºì‹œì— ì €ì¥ (ë‚˜ì¤‘ì— LLMì—ê²Œ ì „ë‹¬)
        self._mathematical_constraints = all_constraints
        self._duplicate_columns = all_duplicates
        self._categorical_insights = all_categorical
        self._derived_columns = all_derived

        return "\n\n".join(summary_parts)

    def _prepare_data_quality_warnings_for_llm(self) -> str:
        """v12.0: LLMì—ê²Œ ì „ë‹¬í•  ë°ì´í„° í’ˆì§ˆ ê²½ê³  ë©”ì‹œì§€"""
        warnings = []

        # ìˆ˜í•™ì  ì œì•½ì¡°ê±´
        if hasattr(self, '_mathematical_constraints') and self._mathematical_constraints:
            warnings.append("## âš ï¸ ìˆ˜í•™ì  ì œì•½ì¡°ê±´ (ë¶„ì„ì—ì„œ ì œì™¸ í•„ìš”)")
            for c in self._mathematical_constraints:
                warnings.append(f"  - {c['reason']}")
                warnings.append(f"    â†’ ì´ ë‘ ì»¬ëŸ¼ì˜ ìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ ìˆ˜í•™ì  ì •ì˜ì…ë‹ˆë‹¤. ì¸ì‚¬ì´íŠ¸ì—ì„œ ì œì™¸í•˜ì„¸ìš”.")

        # ë™ì¼ ë°ì´í„° ì»¬ëŸ¼
        if hasattr(self, '_duplicate_columns') and self._duplicate_columns:
            warnings.append("\n## âš ï¸ ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ (ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ)")
            for d in self._duplicate_columns:
                warnings.append(f"  - {d['reason']}")
                warnings.append(f"    â†’ ì´ ë‘ ì»¬ëŸ¼ì€ ì‚¬ì‹¤ìƒ ë™ì¼í•œ ë°ì´í„°ì…ë‹ˆë‹¤. ë¶„ì„ ê°€ì¹˜ê°€ ì—†ìœ¼ë©°, ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë¡œ ë³´ê³ í•´ì•¼ í•©ë‹ˆë‹¤.")

        # v26.0: íŒŒìƒ ì»¬ëŸ¼
        if hasattr(self, '_derived_columns') and self._derived_columns:
            warnings.append("\n## ğŸ“ íŒŒìƒ(ê³„ì‚°) ì»¬ëŸ¼ ê°ì§€")
            for d in self._derived_columns:
                warnings.append(f"  - {d['reason']}")
                warnings.append(f"    â†’ ì´ ì»¬ëŸ¼ì€ ë‹¤ë¥¸ ì»¬ëŸ¼ì—ì„œ ê³„ì‚°ëœ íŒŒìƒ ê°’ì…ë‹ˆë‹¤. KPI ë¶„ì„ ì‹œ ì›ë³¸ ì»¬ëŸ¼ê³¼ì˜ ìƒê´€ê´€ê³„ëŠ” ë¬´ì˜ë¯¸í•©ë‹ˆë‹¤.")

        # v26.0: êµë€ë³€ìˆ˜ ê²½ê³ 
        if hasattr(self, '_confounding_results') and self._confounding_results:
            warnings.append("\n## ğŸ” êµë€ë³€ìˆ˜ ê°ì§€ (Simpson's Paradox ìœ„í—˜)")
            for cf in self._confounding_results[:5]:
                warnings.append(f"  - {cf['note']}")
                if cf.get('is_simpsons_paradox'):
                    warnings.append(f"    âš ï¸ Simpson's Paradox í™•ì¸ â€” ì¸µí™” ë¶„ì„ì—ì„œ íš¨ê³¼ê°€ ë°˜ì „ë©ë‹ˆë‹¤!")
                warnings.append(f"    â†’ ì´ ìƒê´€ê´€ê³„ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ ë³´ê³ í•˜ì§€ ë§ˆì„¸ìš”. êµë€ë³€ìˆ˜ë¥¼ í†µì œí•œ í¸ìƒê´€: {cf['partial_correlation']:.4f}")

        return "\n".join(warnings) if warnings else ""

    def _prepare_categorical_summary_for_llm(self) -> str:
        """v12.0: LLMì—ê²Œ ì „ë‹¬í•  ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼"""
        if not hasattr(self, '_categorical_insights') or not self._categorical_insights:
            return ""

        summary = ["## ğŸ“Š ì¹´í…Œê³ ë¦¬/ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ê²°ê³¼"]

        for cat in self._categorical_insights[:5]:  # ìµœëŒ€ 5ê°œ
            summary.append(f"\n### {cat['category_column']}ë³„ {cat['metric_column']} ë¶„ì„")
            summary.append(f"  - ì´ {cat['total_categories']}ê°œ ì¹´í…Œê³ ë¦¬")
            summary.append(f"  - ìƒìœ„ 5ê°œê°€ {cat['top5_concentration']:.1f}% ì°¨ì§€")
            summary.append("  - TOP 5:")
            for top in cat['top_categories'][:5]:
                summary.append(f"    â€¢ {top['category']}: {top['value']:.0f} ({top['percentage']:.1f}%)")

        return "\n".join(summary)

    def _filter_invalid_correlations(self, correlations: List[CorrelationResult]) -> List[CorrelationResult]:
        """
        v12.1: ë¬´íš¨í•œ ìƒê´€ê´€ê³„ í•„í„°ë§ (ìˆ˜í•™ì  ì œì•½ì¡°ê±´, ë™ì¼ ë°ì´í„°)

        LLMì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì½”ë“œ ë ˆë²¨ì—ì„œ ì§ì ‘ í•„í„°ë§
        - ìˆ˜í•™ì  ì œì•½ì¡°ê±´ ì»¬ëŸ¼ ìŒ (A+B=1, A+B=100)
        - ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ ìŒ (100% ì¼ì¹˜)
        - ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ì´ 0.99 ì´ìƒ (ê±°ì˜ ì™„ë²½í•œ ìƒê´€ê´€ê³„ = ìˆ˜í•™ì  ê´€ê³„ ì˜ì‹¬)
        """
        if not correlations:
            return correlations

        # í•„í„°ë§ ëŒ€ìƒ ì»¬ëŸ¼ ìŒ ìˆ˜ì§‘
        excluded_pairs = set()

        # 1. ìˆ˜í•™ì  ì œì•½ì¡°ê±´ ì»¬ëŸ¼ ìŒ ìˆ˜ì§‘
        if hasattr(self, '_mathematical_constraints') and self._mathematical_constraints:
            for constraint in self._mathematical_constraints:
                col1 = constraint.get('column_1', '')
                col2 = constraint.get('column_2', '')
                table = constraint.get('table', '')
                if col1 and col2:
                    # ì–‘ë°©í–¥ ì¶”ê°€
                    excluded_pairs.add((table, col1, table, col2))
                    excluded_pairs.add((table, col2, table, col1))
                    # ë‹¤ë¥¸ í…Œì´ë¸”ì—ì„œë„ ê°™ì€ ì´ë¦„ì˜ ì»¬ëŸ¼ì´ë©´ ì œì™¸
                    excluded_pairs.add(('*', col1, '*', col2))
                    excluded_pairs.add(('*', col2, '*', col1))
                    logger.debug(f"[v12.1] Excluding math constraint pair: {col1} <-> {col2}")

        # 2. ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ ìŒ ìˆ˜ì§‘
        if hasattr(self, '_duplicate_columns') and self._duplicate_columns:
            for dup in self._duplicate_columns:
                col1 = dup.get('column_1', '')
                col2 = dup.get('column_2', '')
                table = dup.get('table', '')
                if col1 and col2:
                    excluded_pairs.add((table, col1, table, col2))
                    excluded_pairs.add((table, col2, table, col1))
                    excluded_pairs.add(('*', col1, '*', col2))
                    excluded_pairs.add(('*', col2, '*', col1))
                    logger.debug(f"[v12.1] Excluding duplicate pair: {col1} <-> {col2}")

        # 3. í•„í„°ë§ ìˆ˜í–‰
        filtered = []
        removed_count = 0

        for corr in correlations:
            # ìƒê´€ê³„ìˆ˜ê°€ ê±°ì˜ ì™„ë²½í•œ ê²½ìš° (|r| >= 0.99) - ìˆ˜í•™ì  ê´€ê³„ ì˜ì‹¬
            if abs(corr.correlation) >= 0.99:
                # ì´ë¦„ íŒ¨í„´ ê¸°ë°˜ ì¶”ê°€ ê²€ì¦
                src_col_lower = corr.source_column.lower()
                tgt_col_lower = corr.target_column.lower()

                # "ë¹„ìœ¨" ì»¬ëŸ¼ ìŒì´ë©´ì„œ -1.0 ìƒê´€ê³„ìˆ˜ë©´ A+B=1 ê´€ê³„
                if abs(corr.correlation + 1.0) < 0.01:  # -1.0ì— ê°€ê¹Œì›€
                    if ('ë¹„ìœ¨' in src_col_lower and 'ë¹„ìœ¨' in tgt_col_lower) or \
                       ('rate' in src_col_lower and 'rate' in tgt_col_lower) or \
                       ('ratio' in src_col_lower and 'ratio' in tgt_col_lower) or \
                       ('percent' in src_col_lower and 'percent' in tgt_col_lower):
                        logger.warning(f"[v12.1] REMOVED (math constraint pattern): "
                                      f"{corr.source_column} <-> {corr.target_column} (r={corr.correlation:.3f})")
                        removed_count += 1
                        continue

                # ì´ë¦„ì´ ë§¤ìš° ìœ ì‚¬í•œ ê²½ìš°ë„ í•„í„°ë§ (ì˜ˆ: ëª¨ë°”ì¼_ì„¸ì…˜ìˆ˜ vs ì „ì²´_ì„¸ì…˜ìˆ˜)
                if abs(corr.correlation - 1.0) < 0.01:  # +1.0ì— ê°€ê¹Œì›€
                    # ê³µí†µ ë¶€ë¶„ í™•ì¸ (ì˜ˆ: "ì„¸ì…˜ìˆ˜"ê°€ ë‘˜ ë‹¤ í¬í•¨)
                    common_parts = set(src_col_lower.split('_')) & set(tgt_col_lower.split('_'))
                    if common_parts - {'', 'ì „ì²´', 'ëª¨ë°”ì¼', 'pc', 'total', 'mobile'}:
                        logger.warning(f"[v12.1] REMOVED (duplicate pattern): "
                                      f"{corr.source_column} <-> {corr.target_column} (r={corr.correlation:.3f})")
                        removed_count += 1
                        continue

            # ëª…ì‹œì ìœ¼ë¡œ ì œì™¸ëœ ìŒì¸ì§€ í™•ì¸
            pair1 = (corr.source_table, corr.source_column, corr.target_table, corr.target_column)
            pair2 = ('*', corr.source_column, '*', corr.target_column)

            if pair1 in excluded_pairs or pair2 in excluded_pairs:
                logger.warning(f"[v12.1] REMOVED (explicit exclusion): "
                              f"{corr.source_table}.{corr.source_column} <-> "
                              f"{corr.target_table}.{corr.target_column}")
                removed_count += 1
                continue

            # ìœ íš¨í•œ ìƒê´€ê´€ê³„ë§Œ ì¶”ê°€
            filtered.append(corr)

        if removed_count > 0:
            logger.info(f"[v12.1] Filtered out {removed_count} invalid correlations")

        return filtered

    def _create_data_quality_insights_only(self) -> List[PalantirInsight]:
        """
        v13.0: Phase 1 ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë°ì´í„° í’ˆì§ˆ + ì¹´í…Œê³ ë¦¬ ì¸ì‚¬ì´íŠ¸ ë°˜í™˜

        v13.0 ë³€ê²½:
        - Phase 1 DataAnalystAgentì˜ ë¶„ì„ ê²°ê³¼(data_patterns, data_quality_issues)ë¥¼ í™œìš©
        - recommended_analysisì˜ key_dimensions, key_metrics ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
        - LLMì´ ì´ë¯¸ íƒì§€í•œ íŒ¨í„´ì„ ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜

        ëª¨ë“  ìƒê´€ê´€ê³„ê°€ ìˆ˜í•™ì  ì œì•½ì¡°ê±´ì´ë‚˜ ë™ì¼ ë°ì´í„°ë¡œ í•„í„°ë§ëœ ê²½ìš°ì—ë„,
        ì¹´í…Œê³ ë¦¬/ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ê²°ê³¼ëŠ” ìœ íš¨í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ì´ë¯€ë¡œ í¬í•¨í•©ë‹ˆë‹¤.
        """
        insights = []
        insight_id = 1

        # v13.0: Phase 1ì—ì„œ LLMì´ íƒì§€í•œ data_patternsë¥¼ ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜
        if hasattr(self, '_phase1_analysis') and self._phase1_analysis:
            data_patterns = self._phase1_analysis.get('data_patterns', [])
            for pattern in data_patterns[:3]:  # ìµœëŒ€ 3ê°œ
                pattern_type = pattern.get('pattern_type', '')
                description = pattern.get('description', '')
                implication = pattern.get('implication', '')
                columns = pattern.get('columns_involved', [])

                if pattern_type == 'mathematical_constraint':
                    # ìˆ˜í•™ì  ì œì•½ì¡°ê±´ íŒ¨í„´
                    insights.append(PalantirInsight(
                        insight_id=f"DQ_{insight_id:03d}",
                        insight_type="data_quality_issue",
                        source_table="",
                        source_column=columns[0] if columns else "",
                        source_condition=f"LLM íƒì§€: {description}",
                        target_table="",
                        target_column=columns[1] if len(columns) > 1 else "",
                        predicted_impact=implication,
                        correlation_coefficient=-1.0,
                        p_value=0.0,
                        confidence=1.0,
                        sample_size=0,
                        business_interpretation=f"[LLM ë¶„ì„] {description}. {implication}",
                        recommended_action="ì´ ì»¬ëŸ¼ ìŒì€ ìƒê´€ê´€ê³„ ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.",
                        risk_assessment="ìˆ˜í•™ì  ê´€ê³„ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ ì˜¤í•´í•˜ì§€ ë§ˆì„¸ìš”.",
                        opportunity_assessment="ì„¸ê·¸ë¨¼íŠ¸/ì¹´í…Œê³ ë¦¬ ë¶„ì„ì— ì§‘ì¤‘í•˜ì„¸ìš”.",
                    ))
                    insight_id += 1
                elif pattern_type == 'duplicate_column':
                    # ì¤‘ë³µ ì»¬ëŸ¼ íŒ¨í„´
                    insights.append(PalantirInsight(
                        insight_id=f"DQ_{insight_id:03d}",
                        insight_type="data_quality_issue",
                        source_table="",
                        source_column=columns[0] if columns else "",
                        source_condition=f"LLM íƒì§€: {description}",
                        target_table="",
                        target_column=columns[1] if len(columns) > 1 else "",
                        predicted_impact=implication,
                        correlation_coefficient=1.0,
                        p_value=0.0,
                        confidence=1.0,
                        sample_size=0,
                        business_interpretation=f"[LLM ë¶„ì„] {description}",
                        recommended_action="ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ê²€í† í•˜ì„¸ìš”.",
                        risk_assessment="ì¤‘ë³µ ë°ì´í„°ë¡œ ì¸í•´ ë¶„ì„ ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                        opportunity_assessment="ë°ì´í„° í’ˆì§ˆ ê°œì„ ì„ í†µí•´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                    ))
                    insight_id += 1

            # v13.0: Phase 1ì—ì„œ íƒì§€í•œ data_quality_issuesë¥¼ ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜
            quality_issues = self._phase1_analysis.get('data_quality_issues', [])
            for issue in quality_issues[:2]:  # ìµœëŒ€ 2ê°œ
                insights.append(PalantirInsight(
                    insight_id=f"DQ_{insight_id:03d}",
                    insight_type="data_quality_issue",
                    source_table="",
                    source_column=issue.get('columns', [''])[0] if issue.get('columns') else "",
                    source_condition=f"LLM íƒì§€: {issue.get('issue_type', '')}",
                    target_table="",
                    target_column="",
                    predicted_impact=issue.get('description', ''),
                    correlation_coefficient=0.0,
                    p_value=0.0,
                    confidence=0.9,
                    sample_size=0,
                    business_interpretation=f"[LLM ë¶„ì„] {issue.get('description', '')}",
                    recommended_action="ë°ì´í„° í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    risk_assessment=f"ì‹¬ê°ë„: {issue.get('severity', 'unknown')}",
                    opportunity_assessment="í’ˆì§ˆ ê°œì„ ìœ¼ë¡œ ë¶„ì„ ì •í™•ë„ í–¥ìƒ ê°€ëŠ¥",
                ))
                insight_id += 1

            logger.info(f"[v13.0] Created {insight_id - 1} insights from Phase 1 analysis")

        # v12.2: ì¹´í…Œê³ ë¦¬/ì„¸ê·¸ë¨¼íŠ¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ë¨¼ì € ì¶”ê°€ (ê°€ì¥ ê°€ì¹˜ìˆëŠ” ì¸ì‚¬ì´íŠ¸)
        # (Phase 1 ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì¹´í…Œê³ ë¦¬ ì¸ì‚¬ì´íŠ¸ê°€ ì¶”ê°€ë¡œ í•„ìš”í•œ ê²½ìš°)
        if hasattr(self, '_categorical_insights') and self._categorical_insights:
            for cat in self._categorical_insights[:5]:  # ìµœëŒ€ 5ê°œ
                top_cats = cat.get('top_categories', [])
                if not top_cats:
                    continue

                # TOP ì¹´í…Œê³ ë¦¬ ìš”ì•½
                top_summary = ", ".join([f"{c['category']}({c['percentage']:.1f}%)" for c in top_cats[:3]])

                # ì§‘ì¤‘ë„ ë¶„ì„
                concentration = cat.get('top5_concentration', 0)
                concentration_level = "ë§¤ìš° ë†’ìŒ" if concentration > 80 else "ë†’ìŒ" if concentration > 60 else "ë³´í†µ"

                insights.append(PalantirInsight(
                    insight_id=f"SEG_{insight_id:03d}",
                    insight_type="segment_analysis",
                    source_table=cat.get('table', ''),
                    source_column=cat.get('category_column', ''),
                    source_condition=f"{cat.get('category_column', '')}ë³„ {cat.get('metric_column', '')} ë¶„ì„",
                    target_table=cat.get('table', ''),
                    target_column=cat.get('metric_column', ''),
                    predicted_impact=f"ìƒìœ„ 5ê°œ {cat.get('category_column', '')}ê°€ ì „ì²´ì˜ {concentration:.1f}%ë¥¼ ì°¨ì§€ ({concentration_level} ì§‘ì¤‘ë„)",
                    correlation_coefficient=concentration / 100,  # ì§‘ì¤‘ë„ë¥¼ 0-1 ìŠ¤ì¼€ì¼ë¡œ
                    p_value=0.0,
                    confidence=0.9,
                    sample_size=cat.get('total_categories', 0),
                    business_interpretation=f"{cat.get('metric_column', '')} ê¸°ì¤€ ìƒìœ„ ìœ ì…ê²½ë¡œ: {top_summary}. "
                                           f"ì´ {cat.get('total_categories', 0)}ê°œ {cat.get('category_column', '')} ì¤‘ "
                                           f"ìƒìœ„ 5ê°œê°€ {concentration:.1f}%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.",
                    recommended_action=f"ìƒìœ„ {cat.get('category_column', '')}({top_cats[0]['category'] if top_cats else ''})ì— ëŒ€í•œ "
                                      f"ë§ˆì¼€íŒ… íˆ¬ì ê°•í™” ë° í•˜ìœ„ ì±„ë„ íš¨ìœ¨ì„± ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    risk_assessment=f"{'ì†Œìˆ˜ ì±„ë„ ì˜ì¡´ë„ê°€ ë†’ì•„ ë¦¬ìŠ¤í¬ ë¶„ì‚° í•„ìš”' if concentration > 70 else 'ì±„ë„ ë¶„ì‚°ì´ ì–‘í˜¸í•©ë‹ˆë‹¤'}",
                    opportunity_assessment=f"{'ê³ ì§‘ì¤‘ ì±„ë„ ìµœì í™”ë¡œ ROI ê·¹ëŒ€í™” ê°€ëŠ¥' if concentration > 70 else 'ë‹¤ì–‘í•œ ì±„ë„ì—ì„œ ì„±ì¥ ê¸°íšŒ ì¡´ì¬'}",
                ))
                insight_id += 1

        # ìˆ˜í•™ì  ì œì•½ì¡°ê±´ ì¸ì‚¬ì´íŠ¸ (ë°ì´í„° í’ˆì§ˆ)
        if hasattr(self, '_mathematical_constraints') and self._mathematical_constraints:
            for c in self._mathematical_constraints[:2]:  # ìµœëŒ€ 2ê°œ (ì¹´í…Œê³ ë¦¬ ì¸ì‚¬ì´íŠ¸ ìš°ì„ )
                insights.append(PalantirInsight(
                    insight_id=f"DQ_{insight_id:03d}",
                    insight_type="data_quality_issue",
                    source_table=c.get('table', ''),
                    source_column=c.get('column_1', ''),
                    source_condition=f"ìˆ˜í•™ì  ì œì•½ì¡°ê±´: {c.get('column_1', '')} + {c.get('column_2', '')} = {c.get('sum_value', 0):.2f}",
                    target_table=c.get('table', ''),
                    target_column=c.get('column_2', ''),
                    predicted_impact="ì´ ë‘ ì»¬ëŸ¼ì˜ ìƒê´€ê´€ê³„ëŠ” ìˆ˜í•™ì  ì •ì˜ ê´€ê³„ì´ë©°, ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹™ë‹ˆë‹¤.",
                    correlation_coefficient=-1.0,
                    p_value=0.0,
                    confidence=1.0,
                    sample_size=0,
                    business_interpretation=f"{c.get('column_1', '')}ê³¼ {c.get('column_2', '')}ëŠ” í•©ì´ í•­ìƒ {c.get('sum_value', 0):.2f}ì¸ ë³´ì™„ ê´€ê³„ì…ë‹ˆë‹¤. ë¶„ì„ ê°€ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    recommended_action="ì´ ì»¬ëŸ¼ ìŒì€ ìƒê´€ê´€ê³„ ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.",
                    risk_assessment="ìˆ˜í•™ì  ê´€ê³„ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ ì˜¤í•´í•˜ì§€ ë§ˆì„¸ìš”.",
                    opportunity_assessment="ì„¸ê·¸ë¨¼íŠ¸/ì¹´í…Œê³ ë¦¬ ë¶„ì„ì— ì§‘ì¤‘í•˜ì„¸ìš”.",
                ))
                insight_id += 1

        # ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ ì¸ì‚¬ì´íŠ¸ (ë°ì´í„° í’ˆì§ˆ)
        if hasattr(self, '_duplicate_columns') and self._duplicate_columns:
            for d in self._duplicate_columns[:2]:  # ìµœëŒ€ 2ê°œ
                insights.append(PalantirInsight(
                    insight_id=f"DQ_{insight_id:03d}",
                    insight_type="data_quality_issue",
                    source_table=d.get('table', ''),
                    source_column=d.get('column_1', ''),
                    source_condition=f"ë™ì¼ ë°ì´í„°: {d.get('column_1', '')}ê³¼ {d.get('column_2', '')}ê°€ {d.get('match_rate', 0)*100:.1f}% ì¼ì¹˜",
                    target_table=d.get('table', ''),
                    target_column=d.get('column_2', ''),
                    predicted_impact="ë‘ ì»¬ëŸ¼ì´ ì‚¬ì‹¤ìƒ ë™ì¼í•œ ë°ì´í„°ì…ë‹ˆë‹¤.",
                    correlation_coefficient=1.0,
                    p_value=0.0,
                    confidence=1.0,
                    sample_size=0,
                    business_interpretation=f"'{d.get('column_1', '')}'ê³¼ '{d.get('column_2', '')}'ê°€ ë™ì¼í•©ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜ ë˜ëŠ” ì˜ë„ì  ì¤‘ë³µì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    recommended_action="ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ê²€í† í•˜ì„¸ìš”.",
                    risk_assessment="ì¤‘ë³µ ë°ì´í„°ë¡œ ì¸í•´ ë¶„ì„ ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    opportunity_assessment="ë°ì´í„° í’ˆì§ˆ ê°œì„ ì„ í†µí•´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                ))
                insight_id += 1

        # ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        if not insights:
            insights.append(PalantirInsight(
                insight_id="INFO_001",
                insight_type="no_insights",
                source_table="",
                source_column="",
                source_condition="ë¶„ì„ ê°€ëŠ¥í•œ íŒ¨í„´ì„ ì°¾ì§€ ëª»í•¨",
                target_table="",
                target_column="",
                predicted_impact="ì¶”ê°€ ë°ì´í„° ì†ŒìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                correlation_coefficient=0.0,
                p_value=1.0,
                confidence=0.0,
                sample_size=0,
                business_interpretation="í˜„ì¬ ë°ì´í„°ì—ì„œ ìœ ì˜ë¯¸í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                recommended_action="ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ ì—°ë™, ì‹œê³„ì—´ ë°ì´í„° ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.",
                risk_assessment="ë¶„ì„ ê°€ëŠ¥í•œ ë°ì´í„° ë¶€ì¡±",
                opportunity_assessment="ë°ì´í„° í™•ì¥ì„ í†µí•œ ì¸ì‚¬ì´íŠ¸ ë°œêµ´",
            ))

        logger.info(f"[v12.2] Created {len(insights)} insights (including segment analysis)")
        return insights

    def _prepare_correlation_summary_for_llm(self, correlations: List[CorrelationResult]) -> str:
        """LLMì—ê²Œ ì „ë‹¬í•  ìƒê´€ê´€ê³„ ìš”ì•½"""
        if not correlations:
            return "ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        lines = []
        for i, corr in enumerate(correlations[:10], 1):
            direction = "ì–‘ì˜" if corr.correlation > 0 else "ìŒì˜"
            strength = "ê°•í•œ" if abs(corr.correlation) > 0.7 else "ì¤‘ê°„" if abs(corr.correlation) > 0.5 else "ì•½í•œ"

            lines.append(
                f"{i}. {corr.source_table}.{corr.source_column} â†” {corr.target_table}.{corr.target_column}\n"
                f"   ìƒê´€ê³„ìˆ˜: {corr.correlation:.3f} ({strength} {direction} ìƒê´€ê´€ê³„)\n"
                f"   p-value: {corr.p_value:.4f}, ìƒ˜í”Œ í¬ê¸°: {corr.sample_size}"
            )

        return "\n".join(lines)

    async def _llm_discover_scenarios(
        self,
        data_summary: str,
        correlation_summary: str,
        domain_context: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        v2.0: LLMì´ ììœ¨ì ìœ¼ë¡œ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ íƒì§€/ì„¤ê³„

        LLMì´ ë°ì´í„°ë¥¼ ë³´ê³ :
        1. ì–´ë–¤ ê´€ê³„ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ì ìœ¼ë¡œ ì¤‘ìš”í•œì§€ íŒë‹¨
        2. ì–´ë–¤ what-if ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ê³„
        3. ì„ê³„ê°’, ë³€í™”ìœ¨ ë“±ì„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìë™ ê²°ì •
        """
        if not self.llm_client:
            logger.warning("LLM client not available, falling back to basic analysis")
            return []

        domain_hint = f"\në„ë©”ì¸: {domain_context}" if domain_context else ""

        prompt = f"""ë‹¹ì‹ ì€ íŒ”ë€í‹°ì–´ Foundry ìŠ¤íƒ€ì¼ì˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë°ì´í„°ì™€ ë°œê²¬ëœ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬, ë¹„ì¦ˆë‹ˆìŠ¤ì— ê°€ì¹˜ìˆëŠ” ì˜ˆì¸¡ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
{data_summary}

## ë°œê²¬ëœ ìƒê´€ê´€ê³„
{correlation_summary}
{domain_hint}

## ìš”ì²­ì‚¬í•­

ë‹¤ìŒ í˜•ì‹ì˜ JSON ë°°ì—´ë¡œ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”:

```json
[
  {{
    "scenario_id": "S001",
    "scenario_type": "threshold_impact" | "percentage_change" | "trend_projection" | "anomaly_effect",
    "priority": "high" | "medium" | "low",
    "source": {{
      "table": "í…Œì´ë¸”ëª…",
      "column": "ì»¬ëŸ¼ëª…",
      "condition_type": "exceeds" | "drops_below" | "increases_by" | "decreases_by" | "reaches",
      "threshold_value": ìˆ«ì ë˜ëŠ” null,
      "change_percentage": ìˆ«ì ë˜ëŠ” null,
      "rationale": "ì´ ì¡°ê±´ì„ ì„ íƒí•œ ì´ìœ  (ë°ì´í„° ê¸°ë°˜)"
    }},
    "expected_impacts": [
      {{
        "table": "ì˜í–¥ë°›ëŠ” í…Œì´ë¸”",
        "column": "ì˜í–¥ë°›ëŠ” ì»¬ëŸ¼",
        "impact_type": "increase" | "decrease" | "volatility",
        "estimated_magnitude": "ì˜ˆìƒ ë³€í™”ëŸ‰ ë˜ëŠ” ë¹„ìœ¨",
        "confidence": 0.0-1.0
      }}
    ],
    "business_significance": "ì´ ì‹œë‚˜ë¦¬ì˜¤ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì¤‘ìš”í•œ ì´ìœ ",
    "recommended_monitoring": "ëª¨ë‹ˆí„°ë§í•´ì•¼ í•  ì§€í‘œ",
    "risk_level": "high" | "medium" | "low",
    "opportunity_potential": "high" | "medium" | "low"
  }}
]
```

## ì¤‘ìš” ê°€ì´ë“œë¼ì¸
1. ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬(í‰ê· , í‘œì¤€í¸ì°¨, ë²”ìœ„)ë¥¼ ê³ ë ¤í•˜ì—¬ í˜„ì‹¤ì ì¸ ì„ê³„ê°’ ì„¤ì •
2. ë‹¨ìˆœ ìƒê´€ê´€ê³„ê°€ ì•„ë‹Œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ê³¼ê´€ê³„ ê´€ì ì—ì„œ í•´ì„
3. ì²´ì¸ íš¨ê³¼ (Aâ†’Bâ†’C) ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ë¶„ì„
4. ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒë¥¼ ëª¨ë‘ ê³ ë ¤
5. ìµœì†Œ 3ê°œ, ìµœëŒ€ 8ê°œì˜ ì‹œë‚˜ë¦¬ì˜¤ ì œì•ˆ

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

        try:
            response = await self.llm_client.chat.completions.create(
                model=get_service_model("cross_entity_scenario"),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000,
            )

            content = response.choices[0].message.content.strip()

            # JSON ì¶”ì¶œ
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            scenarios = json.loads(content)
            logger.info(f"LLM designed {len(scenarios)} analysis scenarios")
            return scenarios

        except Exception as e:
            logger.warning(f"LLM scenario discovery failed: {e}")
            return []

    async def _llm_run_simulation(
        self,
        scenario: Dict[str, Any],
        tables_data: Dict[str, Any],
        correlation: Optional[CorrelationResult] = None,
    ) -> SimulationResult:
        """
        v2.0: LLM ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰

        ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ what-if ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸:
        1. ì¡°ê±´ì— ë§ëŠ” ë°ì´í„° ì„œë¸Œì…‹ ì¶”ì¶œ
        2. í•´ë‹¹ ì¡°ê±´ì—ì„œ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ì‹¤ì œ ë³€í™” ë¶„ì„
        3. ì˜ˆì¸¡ ì •í™•ë„ ê²€ì¦
        """
        import pandas as pd

        source_info = scenario.get("source", {})
        source_table = source_info.get("table", "")
        source_column = source_info.get("column", "")

        if source_table not in tables_data or source_column not in tables_data[source_table].columns:
            return SimulationResult(
                scenario_id=scenario.get("scenario_id", "unknown"),
                scenario_description="Invalid scenario",
                source_change={},
                predicted_impacts=[],
                validation_score=0.0,
                sample_evidence=[],
            )

        df_source = tables_data[source_table]
        source_series = df_source[source_column].dropna()

        # ì¡°ê±´ì— ë”°ë¥¸ ë°ì´í„° ë¶„í• 
        condition_type = source_info.get("condition_type", "")
        threshold = source_info.get("threshold_value")
        change_pct = source_info.get("change_percentage")

        # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì¦ê±° ìˆ˜ì§‘
        evidence = []
        predicted_impacts = []
        validation_score = 0.0

        if threshold is not None and condition_type in ["exceeds", "drops_below", "reaches"]:
            # ì„ê³„ê°’ ê¸°ë°˜ ë¶„ì„
            if condition_type == "exceeds":
                high_mask = source_series > threshold
                low_mask = source_series <= threshold
            elif condition_type == "drops_below":
                high_mask = source_series >= threshold
                low_mask = source_series < threshold
            else:
                # reaches: ê·¼ì²˜ ê°’
                range_pct = 0.1 * threshold if threshold != 0 else 1
                high_mask = abs(source_series - threshold) <= range_pct
                low_mask = ~high_mask

            if high_mask.sum() > 5 and low_mask.sum() > 5:
                # ê° íƒ€ê²Ÿ ì»¬ëŸ¼ì— ëŒ€í•´ ì˜í–¥ ë¶„ì„
                for impact in scenario.get("expected_impacts", []):
                    target_table = impact.get("table", "")
                    target_column = impact.get("column", "")

                    if target_table in tables_data and target_column in tables_data[target_table].columns:
                        df_target = tables_data[target_table]
                        min_rows = min(len(df_source), len(df_target))

                        target_series = df_target[target_column].iloc[:min_rows]
                        source_mask_trimmed = high_mask.iloc[:min_rows] if len(high_mask) > min_rows else high_mask
                        low_mask_trimmed = low_mask.iloc[:min_rows] if len(low_mask) > min_rows else low_mask

                        try:
                            high_group_mean = target_series[source_mask_trimmed].mean()
                            low_group_mean = target_series[low_mask_trimmed].mean()

                            if low_group_mean != 0:
                                actual_change_pct = ((high_group_mean - low_group_mean) / abs(low_group_mean)) * 100

                                predicted_impacts.append({
                                    "table": target_table,
                                    "column": target_column,
                                    "baseline_mean": float(low_group_mean),
                                    "condition_mean": float(high_group_mean),
                                    "actual_change_pct": float(actual_change_pct),
                                    "high_group_size": int(source_mask_trimmed.sum()),
                                    "low_group_size": int(low_mask_trimmed.sum()),
                                })

                                evidence.append({
                                    "description": f"{source_column}ì´ {threshold}ë¥¼ {'ì´ˆê³¼' if condition_type == 'exceeds' else 'ë¯¸ë§Œ'}í•  ë•Œ, "
                                                   f"{target_column}ì˜ í‰ê· ì´ {low_group_mean:.2f}ì—ì„œ {high_group_mean:.2f}ë¡œ ë³€í™” ({actual_change_pct:+.1f}%)",
                                    "sample_count": int(source_mask_trimmed.sum()),
                                })

                                validation_score = min(1.0, abs(actual_change_pct) / 100 * 0.5 + 0.5)
                        except Exception as e:
                            logger.debug(f"Impact analysis failed for {target_column}: {e}")

        elif change_pct is not None:
            # ë³€í™”ìœ¨ ê¸°ë°˜ ë¶„ì„ (ìƒìœ„/í•˜ìœ„ ë¶„ìœ„ìˆ˜ ë¹„êµ)
            q_low = source_series.quantile(0.25)
            q_high = source_series.quantile(0.75)

            low_mask = source_series <= q_low
            high_mask = source_series >= q_high

            if high_mask.sum() > 5 and low_mask.sum() > 5:
                for impact in scenario.get("expected_impacts", []):
                    target_table = impact.get("table", "")
                    target_column = impact.get("column", "")

                    if target_table in tables_data and target_column in tables_data[target_table].columns:
                        df_target = tables_data[target_table]
                        min_rows = min(len(df_source), len(df_target))

                        target_series = df_target[target_column].iloc[:min_rows]
                        high_mask_trimmed = high_mask.iloc[:min_rows]
                        low_mask_trimmed = low_mask.iloc[:min_rows]

                        try:
                            high_group_mean = target_series[high_mask_trimmed].mean()
                            low_group_mean = target_series[low_mask_trimmed].mean()

                            if low_group_mean != 0:
                                actual_change_pct = ((high_group_mean - low_group_mean) / abs(low_group_mean)) * 100

                                predicted_impacts.append({
                                    "table": target_table,
                                    "column": target_column,
                                    "q25_mean": float(low_group_mean),
                                    "q75_mean": float(high_group_mean),
                                    "actual_change_pct": float(actual_change_pct),
                                })

                                evidence.append({
                                    "description": f"{source_column} ìƒìœ„ 25%ì™€ í•˜ìœ„ 25% ë¹„êµ ì‹œ, "
                                                   f"{target_column}ì´ {actual_change_pct:+.1f}% ì°¨ì´",
                                    "sample_count": int(high_mask_trimmed.sum()) + int(low_mask_trimmed.sum()),
                                })

                                validation_score = min(1.0, abs(actual_change_pct) / 100 * 0.5 + 0.5)
                        except Exception as e:
                            logger.debug(f"Quartile analysis failed: {e}")

        return SimulationResult(
            scenario_id=scenario.get("scenario_id", str(uuid.uuid4())[:8]),
            scenario_description=scenario.get("business_significance", ""),
            source_change={
                "table": source_table,
                "column": source_column,
                "condition_type": condition_type,
                "threshold": threshold,
                "change_pct": change_pct,
            },
            predicted_impacts=predicted_impacts,
            validation_score=validation_score,
            sample_evidence=evidence,
        )

    async def _llm_generate_insights(
        self,
        scenarios: List[Dict[str, Any]],
        simulation_results: List[SimulationResult],
        correlations: List[CorrelationResult],
        domain_context: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        v2.0: LLMì´ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„±

        ë‹¨ìˆœ í…œí”Œë¦¿ì´ ì•„ë‹Œ, ì‹¤ì œ ë°ì´í„° ì¦ê±°ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì„ ì¢…í•©í•œ ì¸ì‚¬ì´íŠ¸
        """
        if not self.llm_client:
            return []

        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ìš”ì•½
        sim_summary = []
        for sim in simulation_results:
            if sim.predicted_impacts:
                impacts_str = "; ".join([
                    f"{imp.get('column')}: {imp.get('actual_change_pct', 0):+.1f}%"
                    for imp in sim.predicted_impacts
                ])
                sim_summary.append(
                    f"ì‹œë‚˜ë¦¬ì˜¤ {sim.scenario_id}: {sim.scenario_description}\n"
                    f"  ì¡°ê±´: {sim.source_change}\n"
                    f"  ì‹¤ì œ ì˜í–¥: {impacts_str}\n"
                    f"  ê²€ì¦ ì ìˆ˜: {sim.validation_score:.2f}"
                )

        # ìƒê´€ê´€ê³„ ìš”ì•½
        corr_summary = self._prepare_correlation_summary_for_llm(correlations[:5])

        domain_hint = f"\në„ë©”ì¸: {domain_context}" if domain_context else ""

        prompt = f"""ë‹¹ì‹ ì€ íŒ”ë€í‹°ì–´ ìŠ¤íƒ€ì¼ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì™€ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

## ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ (ì‹¤ì œ ë°ì´í„° ê²€ì¦ë¨)
{chr(10).join(sim_summary) if sim_summary else "ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì—†ìŒ"}

## ë°œê²¬ëœ ìƒê´€ê´€ê³„
{corr_summary}
{domain_hint}

## ìš”ì²­ì‚¬í•­

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

```json
[
  {{
    "insight_id": "PI_001",
    "insight_type": "cross_entity_impact" | "threshold_alert" | "predictive_correlation" | "chain_effect",
    "headline": "í•œ ë¬¸ì¥ìœ¼ë¡œ ëœ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (ì˜ˆ: 'Daily_Capacityê°€ 1000ì„ ì´ˆê³¼í•˜ë©´ ìš´ì†¡ë¹„ìš©ì´ 15% ì¦ê°€í•©ë‹ˆë‹¤')",
    "source_condition": "ì¡°ê±´ ì„¤ëª… (êµ¬ì²´ì ì¸ ìˆ«ì í¬í•¨)",
    "predicted_impact": "ì˜ˆì¸¡ë˜ëŠ” ì˜í–¥ (êµ¬ì²´ì ì¸ ìˆ«ì í¬í•¨)",
    "confidence_level": 0.0-1.0,
    "evidence_summary": "ì´ ì˜ˆì¸¡ì„ ë’·ë°›ì¹¨í•˜ëŠ” ë°ì´í„° ì¦ê±° ìš”ì•½",
    "business_interpretation": {{
      "what_it_means": "ë¹„ì¦ˆë‹ˆìŠ¤ì ìœ¼ë¡œ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”",
      "why_it_matters": "ì™œ ì´ê²ƒì´ ì¤‘ìš”í•œì§€",
      "potential_root_cause": "ê°€ëŠ¥í•œ ê·¼ë³¸ ì›ì¸"
    }},
    "risk_assessment": {{
      "risk_level": "high" | "medium" | "low",
      "potential_negative_outcomes": ["ë¶€ì •ì  ê²°ê³¼ ëª©ë¡"],
      "mitigation_strategies": ["ìœ„í—˜ ì™„í™” ì „ëµ"]
    }},
    "opportunity_assessment": {{
      "opportunity_level": "high" | "medium" | "low",
      "potential_benefits": ["ì ì¬ì  ì´ì  ëª©ë¡"],
      "action_items": ["ì‹¤í–‰ í•­ëª©"]
    }},
    "recommended_actions": [
      {{
        "action": "êµ¬ì²´ì ì¸ ì•¡ì…˜",
        "priority": "immediate" | "short_term" | "long_term",
        "expected_outcome": "ì˜ˆìƒ ê²°ê³¼"
      }}
    ],
    "monitoring_kpis": ["ëª¨ë‹ˆí„°ë§í•´ì•¼ í•  KPI ëª©ë¡"],
    "chain_effects": ["Aâ†’Bâ†’C í˜•íƒœì˜ ì²´ì¸ íš¨ê³¼ê°€ ìˆë‹¤ë©´ ê¸°ìˆ "]
  }}
]
```

## ì¤‘ìš” ê°€ì´ë“œë¼ì¸
1. ì‹¤ì œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ì‹¤ì ì¸ ìˆ˜ì¹˜ ì‚¬ìš©
2. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ ì œì‹œ
3. ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒë¥¼ ê· í˜•ìˆê²Œ ë¶„ì„
4. ë¶ˆí™•ì‹¤ì„±ì„ ì†”ì§í•˜ê²Œ í‘œí˜„ (confidence_level ë°˜ì˜)
5. ì²´ì¸ íš¨ê³¼(Aê°€ Bì— ì˜í–¥, Bê°€ Cì— ì˜í–¥)ê°€ ìˆë‹¤ë©´ ëª…ì‹œ

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = await self.llm_client.chat.completions.create(
                model=get_service_model("cross_entity_interpret"),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000,
            )

            content = response.choices[0].message.content.strip()

            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            insights = json.loads(content)
            logger.info(f"LLM generated {len(insights)} business insights")
            return insights

        except Exception as e:
            logger.warning(f"LLM insight generation failed: {e}")
            return []

    async def analyze(
        self,
        table_names: List[str],
        fk_relations: Optional[List[Dict]] = None,
        entity_matches: Optional[List[Dict]] = None,
        domain_context: Optional[str] = None,
    ) -> List[PalantirInsight]:
        """
        v2.0: ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (LLM ììœ¨ ë¶„ì„)

        1. ë°ì´í„° ë¡œë”© ë° í†µê³„ ê³„ì‚°
        2. ìƒê´€ê´€ê³„ ë¶„ì„
        3. LLM ì‹œë‚˜ë¦¬ì˜¤ íƒì§€/ì„¤ê³„
        4. ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        5. LLM ì¸ì‚¬ì´íŠ¸ ìƒì„±
        """
        import pandas as pd

        # 1. í…Œì´ë¸” ë°ì´í„° ë¡œë“œ
        tables_data: Dict[str, pd.DataFrame] = {}
        for table_name in table_names:
            df = self.load_table_data(table_name)
            if df is not None and len(df) > 0:
                tables_data[table_name] = df

        if not tables_data:
            logger.warning("No valid tables loaded")
            return []

        logger.info(f"Loaded {len(tables_data)} tables for analysis")

        # 2. ìƒê´€ê´€ê³„ ë¶„ì„
        correlations = self.compute_cross_table_correlations(table_names)

        # 3. LLM ì‹œë‚˜ë¦¬ì˜¤ íƒì§€/ì„¤ê³„
        data_summary = self._prepare_data_summary_for_llm(tables_data)
        correlation_summary = self._prepare_correlation_summary_for_llm(correlations)

        scenarios = await self._llm_discover_scenarios(
            data_summary, correlation_summary, domain_context
        )

        # 4. ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        simulation_results = []
        for scenario in scenarios:
            sim_result = await self._llm_run_simulation(
                scenario, tables_data,
                correlations[0] if correlations else None
            )
            if sim_result.predicted_impacts:
                simulation_results.append(sim_result)

        logger.info(f"Completed {len(simulation_results)} simulations with valid results")

        # 5. LLM ì¸ì‚¬ì´íŠ¸ ìƒì„±
        llm_insights = await self._llm_generate_insights(
            scenarios, simulation_results, correlations, domain_context
        )

        # PalantirInsight ê°ì²´ë¡œ ë³€í™˜
        insights = []
        for i, llm_insight in enumerate(llm_insights):
            # ê´€ë ¨ ìƒê´€ê´€ê³„ ì°¾ê¸°
            corr = correlations[i] if i < len(correlations) else None
            sim = simulation_results[i] if i < len(simulation_results) else None

            insight = PalantirInsight(
                insight_id=llm_insight.get("insight_id", f"PI_{i+1:03d}"),
                insight_type=llm_insight.get("insight_type", "cross_entity_impact"),
                source_table=corr.source_table if corr else "",
                source_column=corr.source_column if corr else "",
                source_condition=llm_insight.get("source_condition", ""),
                target_table=corr.target_table if corr else "",
                target_column=corr.target_column if corr else "",
                predicted_impact=llm_insight.get("predicted_impact", ""),
                correlation_coefficient=corr.correlation if corr else 0.0,
                p_value=corr.p_value if corr else 1.0,
                confidence=llm_insight.get("confidence_level", 0.5),
                sample_size=corr.sample_size if corr else 0,
                business_interpretation=json.dumps(
                    llm_insight.get("business_interpretation", {}),
                    ensure_ascii=False
                ),
                recommended_action=json.dumps(
                    llm_insight.get("recommended_actions", []),
                    ensure_ascii=False
                ),
                risk_assessment=json.dumps(
                    llm_insight.get("risk_assessment", {}),
                    ensure_ascii=False
                ),
                opportunity_assessment=json.dumps(
                    llm_insight.get("opportunity_assessment", {}),
                    ensure_ascii=False
                ),
                simulation_results=[asdict(sim)] if sim else [],
                validation_evidence=sim.sample_evidence if sim else [],
                source_entity=corr.source_table.replace("_", " ").title() if corr else "",
                target_entity=corr.target_table.replace("_", " ").title() if corr else "",
                chain_effects=llm_insight.get("chain_effects", []),
            )
            insights.append(insight)

        # ìƒê´€ê´€ê³„ëŠ” ìˆì§€ë§Œ LLM ì¸ì‚¬ì´íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        if correlations and not insights:
            insights = self._create_basic_insights_from_correlations(correlations)

        logger.info(f"Generated {len(insights)} total Palantir-style insights")
        return insights

    def _create_basic_insights_from_correlations(
        self,
        correlations: List[CorrelationResult],
    ) -> List[PalantirInsight]:
        """LLM ì—†ì´ ìƒê´€ê´€ê³„ì—ì„œ ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± (í´ë°±)"""
        insights = []

        for i, corr in enumerate(correlations[:10]):
            # ë°ì´í„° ê¸°ë°˜ ë™ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
            source_stats = corr.source_stats

            # í‘œì¤€í¸ì°¨ ê¸°ë°˜ ë³€í™”ëŸ‰ ê³„ì‚°
            if source_stats:
                std = source_stats.get("std", 0)
                mean = source_stats.get("mean", 1)
                if mean != 0:
                    # 1 í‘œì¤€í¸ì°¨ ë³€í™”ê°€ ëª‡ %ì¸ì§€ ê³„ì‚°
                    one_std_pct = (std / abs(mean)) * 100
                    change_pct = min(50, max(10, one_std_pct))  # 10-50% ë²”ìœ„ë¡œ ì œí•œ
                else:
                    change_pct = 20
            else:
                change_pct = 20

            # ì˜ˆì¸¡ ì˜í–¥ ê³„ì‚°
            predicted_change = abs(corr.correlation * change_pct)
            direction = "ì¦ê°€" if corr.correlation > 0 else "ê°ì†Œ"

            insight = PalantirInsight(
                insight_id=f"PI_{i+1:03d}",
                insight_type="cross_entity_impact",
                source_table=corr.source_table,
                source_column=corr.source_column,
                source_condition=f"{change_pct:.0f}% ë³€í™” ì‹œ",
                target_table=corr.target_table,
                target_column=corr.target_column,
                predicted_impact=f"{predicted_change:.1f}% {direction}",
                correlation_coefficient=corr.correlation,
                p_value=corr.p_value,
                confidence=min(0.95, (1 - corr.p_value) * 0.9),
                sample_size=corr.sample_size,
                business_interpretation=f"{corr.source_column}ì™€ {corr.target_column} ì‚¬ì´ì— "
                                        f"{'ê°•í•œ' if abs(corr.correlation) > 0.7 else 'ì¤‘ê°„ ìˆ˜ì¤€ì˜'} "
                                        f"{'ì–‘ì˜' if corr.correlation > 0 else 'ìŒì˜'} ìƒê´€ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤.",
                recommended_action=f"{corr.source_column} ë³€í™” ì‹œ {corr.target_column} ëª¨ë‹ˆí„°ë§ ê¶Œì¥",
                source_entity=corr.source_table.replace("_", " ").title(),
                target_entity=corr.target_table.replace("_", " ").title(),
            )
            insights.append(insight)

        return insights

    def analyze_sync(
        self,
        table_names: List[str],
        fk_relations: Optional[List[Dict]] = None,
        entity_matches: Optional[List[Dict]] = None,
        domain_context: Optional[str] = None,
        phase1_analysis: Optional[Dict[str, Any]] = None,  # v13.0: Phase 1 ë¶„ì„ ê²°ê³¼
    ) -> List[PalantirInsight]:
        """
        v13.0: LLM-First ë¶„ì„ ê²°ê³¼ í™œìš© + ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë¶„ì„

        v13.0 í•µì‹¬ ë³€ê²½:
        - Phase 1 DataAnalystAgentì˜ ë¶„ì„ ê²°ê³¼(data_patterns, recommended_analysis)ë¥¼ í™œìš©
        - LLMì´ ì´ë¯¸ íƒì§€í•œ íŒ¨í„´/í’ˆì§ˆì´ìŠˆë¥¼ ì¤‘ë³µìœ¼ë¡œ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
        - ì¶”ì²œëœ ë¶„ì„ ë°©í–¥(key_dimensions, key_metrics)ì— ë”°ë¼ ì¸ì‚¬ì´íŠ¸ ìƒì„±

        1. Phase 1 ë¶„ì„ ê²°ê³¼ í™œìš©
        2. ìƒê´€ê´€ê³„ ë¶„ì„ (Phase 1 íŒ¨í„´ ì œì™¸)
        3. ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° ì‹¤í—˜ ìˆ˜í–‰
        4. LLMì´ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
        5. Phase 1 ì¶”ì²œ ë¶„ì„ ë°©í–¥ì— ë”°ë¥¸ ì¸ì‚¬ì´íŠ¸ ë³´ì™„
        """
        import pandas as pd
        from .simulation_engine import SimulationEngine

        # v13.0: Phase 1 ë¶„ì„ ê²°ê³¼ ì €ì¥
        self._phase1_analysis = phase1_analysis or {}
        if phase1_analysis:
            logger.info(f"[v13.0] Received Phase 1 analysis: "
                       f"domain={phase1_analysis.get('data_understanding', {}).get('domain', {}).get('detected', 'unknown')}, "
                       f"patterns={len(phase1_analysis.get('data_patterns', []))}, "
                       f"quality_issues={len(phase1_analysis.get('data_quality_issues', []))}")

            # Phase 1ì—ì„œ íƒì§€í•œ key dimensions/metrics ì‚¬ìš©
            recommended = phase1_analysis.get('recommended_analysis', {})
            self._key_dimensions = recommended.get('key_dimensions', [])
            self._key_metrics = recommended.get('key_metrics', [])
            logger.info(f"[v13.0] Key dimensions: {self._key_dimensions}, Key metrics: {self._key_metrics}")

        # 1. í…Œì´ë¸” ë°ì´í„° ë¡œë“œ
        tables_data: Dict[str, pd.DataFrame] = {}
        for table_name in table_names:
            df = self.load_table_data(table_name)
            if df is not None and len(df) > 0:
                tables_data[table_name] = df

        if not tables_data:
            logger.warning("No valid tables loaded for correlation analysis")
            return []

        logger.info(f"Loaded {len(tables_data)} tables for Cross-Entity Correlation Analysis")

        # v12.0: ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰ (ìˆ˜í•™ì  ì œì•½ì¡°ê±´, ë™ì¼ ë°ì´í„° ê°ì§€)
        logger.info("[v12.0] Running data quality checks...")
        _ = self._prepare_data_summary_for_llm(tables_data)  # ì´ í•¨ìˆ˜ê°€ í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰

        # ê°ì§€ëœ ì´ìŠˆ ë¡œê¹…
        if hasattr(self, '_mathematical_constraints') and self._mathematical_constraints:
            for c in self._mathematical_constraints:
                logger.warning(f"[v12.0] Mathematical constraint: {c['reason']}")

        if hasattr(self, '_duplicate_columns') and self._duplicate_columns:
            for d in self._duplicate_columns:
                logger.warning(f"[v12.0] Duplicate columns: {d['reason']}")

        if hasattr(self, '_categorical_insights') and self._categorical_insights:
            logger.info(f"[v12.0] Found {len(self._categorical_insights)} categorical insights")

        # v26.0: íŒŒìƒ ì»¬ëŸ¼ ë¡œê¹…
        if hasattr(self, '_derived_columns') and self._derived_columns:
            for d in self._derived_columns:
                logger.info(f"[v26.0] Derived column: {d['reason']}")

        # v26.0: êµë€ë³€ìˆ˜ íƒì§€ (ë‹¨ì¼ í…Œì´ë¸” ë‚´)
        self._confounding_results = []
        for table_name, df in tables_data.items():
            numeric_cols = self.get_numeric_columns(df)
            # ì£¼ìš” ìƒê´€ ìŒì— ëŒ€í•´ êµë€ë³€ìˆ˜ ì²´í¬
            for i, col_a in enumerate(numeric_cols[:8]):
                for col_b in numeric_cols[i+1:]:
                    try:
                        from scipy import stats
                        mask = df[[col_a, col_b]].notna().all(axis=1)
                        if mask.sum() < 30:
                            continue
                        corr_val, p_val = stats.pearsonr(df.loc[mask, col_a], df.loc[mask, col_b])
                        if abs(corr_val) > 0.3 and p_val < 0.05:
                            confounders = self.detect_confounding_variables(
                                df, col_a, col_b, numeric_cols
                            )
                            if confounders:
                                self._confounding_results.extend(confounders)
                    except Exception as e:
                        logger.debug(f"Confounding analysis failed for {col_a} vs {col_b}: {e}")

        if self._confounding_results:
            logger.info(f"[v26.0] Found {len(self._confounding_results)} confounding relationships")

        # 2. ìƒê´€ê´€ê³„ ë¶„ì„
        correlations = self.compute_cross_table_correlations(table_names)

        if not correlations:
            logger.info("No significant correlations found - returning data quality insights")
            return self._create_data_quality_insights_only()

        logger.info(f"Found {len(correlations)} significant correlations (before filtering)")

        # v12.1: ìˆ˜í•™ì  ì œì•½ì¡°ê±´ ë° ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ ìŒì„ í•„í„°ë§ (LLM ì˜ì¡´ X, ì½”ë“œì—ì„œ ì§ì ‘ ì œê±°)
        original_count = len(correlations)
        correlations = self._filter_invalid_correlations(correlations)
        filtered_count = original_count - len(correlations)
        logger.info(f"[v12.1] After filtering: {len(correlations)} valid correlations (removed {filtered_count})")

        # v12.1: í•„í„°ë§ í›„ì—ë„ ì²´í¬ - ìœ íš¨í•œ ìƒê´€ê´€ê³„ê°€ ì—†ìœ¼ë©´ ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ë§Œ ë°˜í™˜
        if not correlations:
            logger.info("[v12.1] All correlations filtered out - returning data quality insights only")
            return self._create_data_quality_insights_only()

        # 3. ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ ì‹¤í–‰ (v10.1 NEW!)
        logger.info("Running simulation engine...")
        sim_engine = SimulationEngine(tables_data)
        simulation_report = sim_engine.run_all_experiments(correlations)

        logger.info(f"Simulation complete: {simulation_report.total_experiments} experiments, "
                   f"{len(simulation_report.significant_findings)} significant findings")

        # 4. LLMì—ê²Œ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ì „ë‹¬í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []

        # ì‹¤í—˜ì´ ìˆìœ¼ë©´ (ìœ ì˜ë¯¸í•œ ë°œê²¬ì´ ì—†ë”ë¼ë„) LLMì—ê²Œ ì „ë‹¬
        if self.llm_client and simulation_report.total_experiments > 0:
            logger.info("LLM analyzing simulation results...")
            try:
                # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ LLMì—ê²Œ ì „ë‹¬
                simulation_summary = sim_engine.format_for_llm(simulation_report)
                insights = self._llm_analyze_simulation_results(
                    simulation_summary, correlations, domain_context
                )
                if insights:
                    logger.info(f"LLM generated {len(insights)} insights from simulation")
                    return insights
            except Exception as e:
                logger.warning(f"LLM simulation analysis failed: {e}")
                import traceback
                logger.debug(traceback.format_exc())

        # 5. í´ë°±: ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self._create_basic_insights_from_correlations(correlations)

        logger.info(f"Generated {len(insights)} Palantir-style insights (sync mode)")
        return insights

    def _sync_llm_analysis(
        self,
        tables_data: Dict[str, Any],
        correlations: List[CorrelationResult],
        domain_context: Optional[str] = None,
    ) -> List[PalantirInsight]:
        """ë™ê¸° ë°©ì‹ì˜ LLM ë¶„ì„"""
        if not self.llm_client:
            return []

        # ë°ì´í„° ìš”ì•½ ì¤€ë¹„
        data_summary = self._prepare_data_summary_for_llm(tables_data)
        corr_summary = self._prepare_correlation_summary_for_llm(correlations)

        domain_hint = f"\në„ë©”ì¸: {domain_context}" if domain_context else ""

        # ë‹¨ì¼ í†µí•© í”„ë¡¬í”„íŠ¸ë¡œ ì‹œë‚˜ë¦¬ì˜¤ + ì¸ì‚¬ì´íŠ¸ ìƒì„±
        prompt = f"""ë‹¹ì‹ ì€ íŒ”ë€í‹°ì–´ Foundry ìŠ¤íƒ€ì¼ì˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë°ì´í„°ì™€ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ì— ê°€ì¹˜ìˆëŠ” ì˜ˆì¸¡ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ìš”ì•½
{data_summary}

## ë°œê²¬ëœ ìƒê´€ê´€ê³„
{corr_summary}
{domain_hint}

## ìš”ì²­ì‚¬í•­

ê° ìƒê´€ê´€ê³„ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

```json
[
  {{
    "insight_id": "PI_001",
    "insight_type": "cross_entity_impact",
    "source_table": "ì†ŒìŠ¤ í…Œì´ë¸”ëª…",
    "source_column": "ì†ŒìŠ¤ ì»¬ëŸ¼ëª…",
    "target_table": "íƒ€ê²Ÿ í…Œì´ë¸”ëª…",
    "target_column": "íƒ€ê²Ÿ ì»¬ëŸ¼ëª…",
    "source_condition": "êµ¬ì²´ì ì¸ ì¡°ê±´ (ë°ì´í„° ë¶„í¬ ê¸°ë°˜, ì˜ˆ: 'Daily_Capacityê°€ í‰ê·  ëŒ€ë¹„ 1 í‘œì¤€í¸ì°¨ ì¦ê°€ ì‹œ')",
    "predicted_impact": "êµ¬ì²´ì ì¸ ì˜í–¥ ì˜ˆì¸¡ (ì˜ˆ: 'Max_Weight_Quantê°€ ì•½ 12% ì¦ê°€')",
    "confidence": 0.0-1.0,
    "business_interpretation": "ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì˜ í•´ì„",
    "risk_assessment": "ë¦¬ìŠ¤í¬ í‰ê°€",
    "opportunity_assessment": "ê¸°íšŒ í‰ê°€",
    "recommended_actions": ["êµ¬ì²´ì ì¸ ê¶Œì¥ ì•¡ì…˜ ëª©ë¡"],
    "chain_effects": ["Aâ†’Bâ†’C í˜•íƒœì˜ ì—°ì‡„ íš¨ê³¼ (ìˆë‹¤ë©´)"],
    "simulation_scenarios": [
      {{
        "scenario": "ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…",
        "condition": "ì¡°ê±´",
        "expected_outcome": "ì˜ˆìƒ ê²°ê³¼"
      }}
    ]
  }}
]
```

## ì¤‘ìš” ê°€ì´ë“œë¼ì¸
1. ë°ì´í„°ì˜ ì‹¤ì œ í†µê³„(í‰ê· , í‘œì¤€í¸ì°¨, ë²”ìœ„)ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì‹¤ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ê³„
2. 20%ë§Œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ë°ì´í„° ë¶„í¬ì— ë§ëŠ” ë‹¤ì–‘í•œ ë³€í™”ìœ¨ ì‚¬ìš©
3. ì„ê³„ê°’ ì‹œë‚˜ë¦¬ì˜¤ë„ í¬í•¨ (ì˜ˆ: "Xê°€ Yë¥¼ ì´ˆê³¼í•˜ë©´...")
4. ì²´ì¸ íš¨ê³¼ íƒì§€ (Aâ†’Bâ†’C)
5. ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒë¥¼ ê· í˜•ìˆê²Œ ë¶„ì„
6. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = self.llm_client.chat.completions.create(
                model=get_service_model("cross_entity_scenario"),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000,
            )

            content = response.choices[0].message.content.strip()

            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            llm_insights = json.loads(content)

            # PalantirInsight ê°ì²´ë¡œ ë³€í™˜
            insights = []
            for llm_insight in llm_insights:
                # ë§¤ì¹­ë˜ëŠ” ìƒê´€ê´€ê³„ ì°¾ê¸°
                corr = None
                for c in correlations:
                    if (c.source_table == llm_insight.get("source_table") and
                        c.source_column == llm_insight.get("source_column")):
                        corr = c
                        break

                insight = PalantirInsight(
                    insight_id=llm_insight.get("insight_id", f"PI_{len(insights)+1:03d}"),
                    insight_type=llm_insight.get("insight_type", "cross_entity_impact"),
                    source_table=llm_insight.get("source_table", ""),
                    source_column=llm_insight.get("source_column", ""),
                    source_condition=llm_insight.get("source_condition", ""),
                    target_table=llm_insight.get("target_table", ""),
                    target_column=llm_insight.get("target_column", ""),
                    predicted_impact=llm_insight.get("predicted_impact", ""),
                    correlation_coefficient=corr.correlation if corr else 0.0,
                    p_value=corr.p_value if corr else 1.0,
                    confidence=llm_insight.get("confidence", 0.5),
                    sample_size=corr.sample_size if corr else 0,
                    business_interpretation=llm_insight.get("business_interpretation", ""),
                    recommended_action=json.dumps(
                        llm_insight.get("recommended_actions", []),
                        ensure_ascii=False
                    ),
                    risk_assessment=llm_insight.get("risk_assessment", ""),
                    opportunity_assessment=llm_insight.get("opportunity_assessment", ""),
                    simulation_results=llm_insight.get("simulation_scenarios", []),
                    source_entity=llm_insight.get("source_table", "").replace("_", " ").title(),
                    target_entity=llm_insight.get("target_table", "").replace("_", " ").title(),
                    chain_effects=llm_insight.get("chain_effects", []),
                )
                insights.append(insight)

            logger.info(f"Generated {len(insights)} LLM-powered insights")
            return insights

        except Exception as e:
            logger.warning(f"Sync LLM analysis failed: {e}")
            return []

    def _llm_analyze_simulation_results(
        self,
        simulation_summary: str,
        correlations: List[CorrelationResult],
        domain_context: Optional[str] = None,
    ) -> List[PalantirInsight]:
        """
        v12.0: LLMì´ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ê°•í™”ëœ ììœ¨ íŒë‹¨)

        í•µì‹¬ ì›ì¹™:
        - ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ëŠ” LLMì˜ ë¶„ì„ ë„êµ¬ì¼ ë¿
        - ìµœì¢… ì¶œë ¥ì—ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë§Œ í¬í•¨ (ì‹œë®¬ë ˆì´ì…˜ ìƒì„¸ X)
        - ì‚¬ìš©ìê°€ ë³¼ ë‚´ìš©: headline, business_interpretation, risk, opportunity, actions
        - v12.0: LLMì´ ìŠ¤ìŠ¤ë¡œ ë¬´ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¥¼ í•„í„°ë§
        """
        if not self.llm_client:
            return []

        domain_hint = f"\në„ë©”ì¸: {domain_context}" if domain_context else ""

        # v12.0: ë°ì´í„° í’ˆì§ˆ ê²½ê³  ì¶”ê°€
        data_quality_warnings = self._prepare_data_quality_warnings_for_llm()
        categorical_summary = self._prepare_categorical_summary_for_llm()

        prompt = f"""ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ì‹¤ì œ ë°ì´í„°ë¡œ ìˆ˜í–‰í•œ ì‹œë®¬ë ˆì´ì…˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ì˜ì§„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”.

{simulation_summary}
{domain_hint}

{data_quality_warnings}

{categorical_summary}

## ğŸš¨ v12.0 í•µì‹¬ ì§€ì¹¨ - ë¬´ì˜ë¯¸í•œ ì¸ì‚¬ì´íŠ¸ í•„í„°ë§

ë‹¹ì‹ ì€ ë‹¤ìŒ ìœ í˜•ì˜ "ê°€ì§œ ì¸ì‚¬ì´íŠ¸"ë¥¼ ë°˜ë“œì‹œ ì œì™¸í•´ì•¼ í•©ë‹ˆë‹¤:

### 1. ìˆ˜í•™ì  ì œì•½ì¡°ê±´ (Mathematical Constraints)
- A + B = 1 ë˜ëŠ” A + B = 100 ê´€ê³„ëŠ” **ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹™ë‹ˆë‹¤**
- ì˜ˆ: "ì‹ ê·œë°©ë¬¸ë¹„ìœ¨ì´ ì¦ê°€í•˜ë©´ ì¬ë°©ë¬¸ë¹„ìœ¨ì´ ê°ì†Œ" â†’ âŒ ì œì™¸ (ë‘˜ì˜ í•©ì€ í•­ìƒ 1)
- ì˜ˆ: "ì™„ë£Œìœ¨ì´ ë†’ìœ¼ë©´ ë¯¸ì™„ë£Œìœ¨ì´ ë‚®ë‹¤" â†’ âŒ ì œì™¸ (ìëª…í•œ ì‚¬ì‹¤)
- **ìƒê´€ê³„ìˆ˜ê°€ -1.0 ë˜ëŠ” 1.0ì— ê°€ê¹ë‹¤ë©´ ìˆ˜í•™ì  ê´€ê³„ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤**

### 2. ë™ì¼ ë°ì´í„° (Duplicate Columns)
- ë‘ ì»¬ëŸ¼ì´ 100% ë™ì¼í•˜ë©´ **ë¶„ì„ ê°€ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤**
- ì˜ˆ: "ëª¨ë°”ì¼_ì„¸ì…˜ìˆ˜ì™€ ì „ì²´_ì„¸ì…˜ìˆ˜ê°€ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤" â†’ âŒ ì œì™¸ (ê°™ì€ ë°ì´í„°)
- ëŒ€ì‹  **"ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ: ë‘ ì»¬ëŸ¼ì´ ë™ì¼í•¨"**ìœ¼ë¡œ ë³´ê³ í•˜ì„¸ìš”

### 3. ìëª…í•œ ë™ì–´ë°˜ë³µ (Tautology)
- "Xê°€ ë†’ìœ¼ë©´ X_ê´€ë ¨ì§€í‘œë„ ë†’ë‹¤" â†’ âŒ ì œì™¸
- ì»¬ëŸ¼ ì´ë¦„ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ ê°™ì€ ê°œë…ì„ ì¸¡ì •í•˜ëŠ” ê²½ìš° ì£¼ì˜

### 4. ê°€ì¹˜ ì—†ëŠ” ìƒê´€ê´€ê³„
- ë¹„ì¦ˆë‹ˆìŠ¤ì  ì˜ë¯¸ê°€ ì—†ê±°ë‚˜ ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œì™¸
- "ìƒê´€ê´€ê³„ê°€ ìˆë‹¤"ë§Œìœ¼ë¡œëŠ” ì¸ì‚¬ì´íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤

## âœ… ì¢‹ì€ ì¸ì‚¬ì´íŠ¸ì˜ ì¡°ê±´
1. **ì‹¤í–‰ ê°€ëŠ¥**: ëˆ„êµ°ê°€ê°€ ì´ ì •ë³´ë¡œ í–‰ë™ì„ ì·¨í•  ìˆ˜ ìˆì–´ì•¼ í•¨
2. **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜**: ë§¤ì¶œ, ë¹„ìš©, ë¦¬ìŠ¤í¬, íš¨ìœ¨ì„±ì— ì˜í–¥ì„ ì¤˜ì•¼ í•¨
3. **êµ¬ì²´ì **: "Xê°€ Yë¥¼ ì´ˆê³¼í•˜ë©´ Zê°€ 15% ì¦ê°€"ì²˜ëŸ¼ ìˆ˜ì¹˜ í¬í•¨
4. **ì¹´í…Œê³ ë¦¬ ê¸°ë°˜**: ì„¸ê·¸ë¨¼íŠ¸/íŠ¸ë˜í”½ ì†ŒìŠ¤/ì§€ì—­ë³„ ì°¨ì´ ë¶„ì„ì€ ë§¤ìš° ìœ ìš©

## ì¶œë ¥ í˜•ì‹

```json
[
  {{
    "insight_id": "PI_001",
    "headline": "í•µì‹¬ ë°œê²¬ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ (ì˜ˆ: 'ëŒ€ìš©ëŸ‰ ì°½ê³ ì—ì„œ ìš´ì†¡ ì§€ì—° ìœ„í—˜ 2ë°° ë†’ìŒ')",
    "insight_type": "actionable_insight" | "data_quality_issue" | "category_analysis",
    "business_interpretation": "ì´ê²ƒì´ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì˜ë¯¸í•˜ëŠ” ë°”ì™€ ì›ì¸ ë¶„ì„",
    "why_this_matters": "ì™œ ì´ê²ƒì´ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì¤‘ìš”í•œì§€ (ìëª…í•œ ê´€ê³„ê°€ ì•„ë‹Œ ì´ìœ )",
    "risk_level": "high" | "medium" | "low",
    "risk_description": "êµ¬ì²´ì ì¸ ìœ„í—˜ ìš”ì†Œì™€ ì ì¬ì  ì†ì‹¤",
    "opportunity": "ì´ ë°œê²¬ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ê¸°íšŒ",
    "recommended_actions": [
      "ì¦‰ì‹œ ì‹¤í–‰í•  êµ¬ì²´ì ì¸ ì•¡ì…˜ 1",
      "ë‹¨ê¸°ì ìœ¼ë¡œ ê²€í† í•  ì‚¬í•­ 2"
    ],
    "affected_entities": ["ì˜í–¥ë°›ëŠ” êµ¬ì²´ì ì¸ ì—”í‹°í‹° ëª©ë¡"],
    "urgency": "immediate" | "short_term" | "long_term",
    "source_table": "ê´€ë ¨ ì†ŒìŠ¤ í…Œì´ë¸”",
    "source_column": "ê´€ë ¨ ì†ŒìŠ¤ ì»¬ëŸ¼",
    "target_table": "ê´€ë ¨ íƒ€ê²Ÿ í…Œì´ë¸”",
    "target_column": "ê´€ë ¨ íƒ€ê²Ÿ ì»¬ëŸ¼",
    "excluded_false_insights": ["ì œì™¸í•œ ê°€ì§œ ì¸ì‚¬ì´íŠ¸ì™€ ê·¸ ì´ìœ  (ì„ íƒ)"]
  }}
]
```

## ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ìˆ˜í•™ì  ì œì•½ì¡°ê±´(A+B=1)ì„ ì¸ê³¼ê´€ê³„ë¡œ í•´ì„í•˜ì§€ ì•Šì•˜ëŠ”ê°€?
- [ ] ë™ì¼ ë°ì´í„° ì»¬ëŸ¼ì„ ìƒê´€ê´€ê³„ë¡œ ë³´ê³ í•˜ì§€ ì•Šì•˜ëŠ”ê°€?
- [ ] ê° ì¸ì‚¬ì´íŠ¸ê°€ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ê°€ ìˆëŠ”ê°€?
- [ ] ì¹´í…Œê³ ë¦¬/ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€?

ë»”í•˜ì§€ ì•Šì€, ì‹¤ì œë¡œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ë§Œ ìƒì„±í•˜ì„¸ìš”. JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = self.llm_client.chat.completions.create(
                model=get_service_model("cross_entity_interpret"),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000,
            )

            content = response.choices[0].message.content.strip()

            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            llm_insights = json.loads(content)

            # PalantirInsight ê°ì²´ë¡œ ë³€í™˜
            insights = []
            for llm_insight in llm_insights:
                # ë§¤ì¹­ë˜ëŠ” ìƒê´€ê´€ê³„ ì°¾ê¸°
                corr = None
                for c in correlations:
                    if (c.source_table == llm_insight.get("source_table") and
                        c.source_column == llm_insight.get("source_column")):
                        corr = c
                        break

                insight = PalantirInsight(
                    insight_id=llm_insight.get("insight_id", f"PI_{len(insights)+1:03d}"),
                    insight_type="simulation_based",  # v10.1: ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ì„ì„ ëª…ì‹œ
                    source_table=llm_insight.get("source_table", ""),
                    source_column=llm_insight.get("source_column", ""),
                    source_condition=llm_insight.get("headline", ""),  # headlineì„ conditionìœ¼ë¡œ ì‚¬ìš©
                    target_table=llm_insight.get("target_table", ""),
                    target_column=llm_insight.get("target_column", ""),
                    predicted_impact=llm_insight.get("headline", ""),
                    correlation_coefficient=corr.correlation if corr else 0.0,
                    p_value=corr.p_value if corr else 1.0,
                    confidence=0.85 if llm_insight.get("risk_level") == "high" else 0.7,
                    sample_size=corr.sample_size if corr else 0,
                    business_interpretation=llm_insight.get("business_interpretation", ""),
                    recommended_action=json.dumps(
                        llm_insight.get("recommended_actions", []),
                        ensure_ascii=False
                    ),
                    risk_assessment=llm_insight.get("risk_description", ""),
                    opportunity_assessment=llm_insight.get("opportunity", ""),
                    # v10.1: simulation_resultsëŠ” ë¹„ì›Œë‘  (ì‚¬ìš©ìì—ê²Œ ë…¸ì¶œ X)
                    simulation_results=[],
                    source_entity=llm_insight.get("source_table", "").replace("_", " ").title(),
                    target_entity=llm_insight.get("target_table", "").replace("_", " ").title(),
                    chain_effects=llm_insight.get("affected_entities", []),
                )
                insights.append(insight)

            logger.info(f"Generated {len(insights)} simulation-based insights")
            return insights

        except Exception as e:
            logger.warning(f"LLM simulation analysis failed: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return []
